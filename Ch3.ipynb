{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peterchang0414/SLP/blob/main/Ch3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3: N-gram Language Models"
      ],
      "metadata": {
        "id": "qk47KCC8e3kD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solution by: Peter Chang (GItHub: @peterchang0414)\n",
        "\n",
        "*Last edited: 2021.12.22*"
      ],
      "metadata": {
        "id": "6A8G9Nch76Cq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1**: Write out the equation for trigram probability estimation (modifying Eq. 3.11). Now write out all the non-zero trigram probabilities for the *I am Sam* corpus on page 32."
      ],
      "metadata": {
        "id": "bxhs_F7yfDEK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a trigram model, we make the following approximation:\n",
        "\\begin{align}\n",
        "        P(w_n|w_{1:n-1}) \\approx P(w_n|w_{n-2}w_{n-1})\n",
        "\\end{align}\n",
        "As with the bigram model, to compute the trigram probability of a word $z$ given previous two words $x$ and $y$, we compute the count of the trigram $C(xyz)$ and normalize by ubigram count of $C(xy)$:\n",
        "\\begin{align}\n",
        "  P(w_n|w_{n-2}w_{n-1}) &= \\frac{C(w_{n-2}w_{n-1}w_{n})}{C(w_{n-2}w_{n-1})}\n",
        "\\end{align}\n",
        "\n",
        "For the *I am Sam* corpus, we augment each sentence with two specials at the beginning and at the end:\n",
        "\n",
        "\\begin{align}\n",
        "  &\\texttt{<s><s> I am Sam </s></s>} \\\\\n",
        "  &\\texttt{<s><s> Sam I am </s></s>} \\\\\n",
        "  &\\texttt{<s><s> I do not like green eggs and ham </s></s>}\n",
        "\\end{align}\n",
        "\n",
        "Then, the following are all the non-zero trigram probabilities for the corpus:\n",
        "\n",
        "\\begin{align}\n",
        "  P(\\texttt{I}|\\texttt{<s><s>}) &= \\frac{2}{3} = .67, && &P(\\texttt{Sam}|\\texttt{<s><s>}) &= \\frac{1}{3} = .33 && &P(\\texttt{am}|\\texttt{<s> I}) &= \\frac{1}{2}=.5 \\\\\n",
        "  P(\\texttt{I}|\\texttt{<s> Sam}) &= 1 && &P(\\texttt{do}|\\texttt{<s> I}) &= \\frac{1}{2} = .5 && &P(\\texttt{Sam}|\\texttt{I am}) &= \\frac{1}{2} = .5 \\\\\n",
        "  P(\\texttt{am}|\\texttt{Sam I}) &= 1 && &P(\\texttt{not}|\\texttt{I do}) &= 1 && &P(\\texttt{</s>}|\\texttt{am Sam}) &= 1 \\\\\n",
        "  P(\\texttt{</s>}|\\texttt{I am}) &= \\frac{1}{2} = .5 && &P(\\texttt{like}|\\texttt{do not}) &= 1 && &P(\\texttt{</s>}|\\texttt{Sam </s>}) &= 1 \\\\\n",
        "  P(\\texttt{</s>}|\\texttt{am </s>}) &= 1 && &P(\\texttt{green}|\\texttt{not like}) &= 1 && &P(\\texttt{eggs}|\\texttt{like green}) &= 1 \\\\\n",
        "  P(\\texttt{and}|\\texttt{green eggs}) &= 1 && &P(\\texttt{ham}|\\texttt{eggs and}) &= 1 && &P(\\texttt{</s>}|\\texttt{and ham}) &= 1 \\\\\n",
        "  P(\\texttt{</s>}|\\texttt{ham </s>}) &= 1\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "vkrdKv5_-vzM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.2**: Calculate the probability of the sentence $\\texttt{i want chinese food}$. Give two probabilities, one using Fig. 3.2 and the 'useful probabililties' just below it on page 34, and another using the add-1 smoothed table in Fig. 3.7. Assume the additional add-1 smoothed probabilities $P(\\texttt{i}|\\texttt{<s>}) = 0.19$ and $P(\\texttt{</s>}|\\texttt{food}) = 0.40$."
      ],
      "metadata": {
        "id": "Kg0BkgRuAMsz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Fig 3.2, we can compute the probability of the sentence as follows:\n",
        "\\begin{align}\n",
        "  P(\\texttt{<s> i want chinese food </s>}) &= P(\\texttt{i|<s>})\\cdot P(\\texttt{want|i}) \\cdot P(\\texttt{chinese|want}) \\cdot P(\\texttt{food|chinese}) \\cdot P(\\texttt{</s>|food}) \\\\\n",
        "  &= .25 \\times .33 \\times .0065 \\times .52 \\times .68 \\\\\n",
        "  &\\approx .00019\n",
        "\\end{align}\n",
        "\n",
        "Using the add-1 smoothed probabilities, we can compute the probability of the sentence as follows:\n",
        "\\begin{align}\n",
        "  P(\\texttt{<s> i want chinese food </s>}) &= P(\\texttt{i|<s>})\\cdot P(\\texttt{want|i}) \\cdot P(\\texttt{chinese|want}) \\cdot P(\\texttt{food|chinese}) \\cdot P(\\texttt{</s>|food}) \\\\\n",
        "  &= .19 \\times .21 \\times .0029 \\times .052 \\times .40 \\\\\n",
        "  &\\approx .0000024\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "s6tl1RiTCQg2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.3**: Which of the two probabilities your computed in the previous exercise is higher, unsmoothed or smoothed? Explain why."
      ],
      "metadata": {
        "id": "7ndt4KlSsmhy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The unsmoothed probability is higher than the smoothed probability. This is because all the bigram word pairs in the sentence had non-zero probability in the unsmoothed model, which means the smoothing operation took away probabilities from those pairs to add to the pairs which initially have probabilities of zero."
      ],
      "metadata": {
        "id": "upQfauyNsuED"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.4**: We are given the following corpus, modified from the one in the chapter:\n",
        "\\begin{align}\n",
        "  &\\texttt{<s> I am Sam </s>} \\\\\n",
        "  &\\texttt{<s> Sam I am </s>} \\\\\n",
        "  &\\texttt{<s> I am Sam </s>} \\\\\n",
        "  &\\texttt{<s> I do not like green eggs and Sam </s>} \n",
        "\\end{align}\n",
        "Using a bigram language model with add-one smoothing, what is $P(\\texttt{Sam|am})$? Include $\\texttt{<s>}$ and $\\texttt{</s>}$ in your counts just like any other token."
      ],
      "metadata": {
        "id": "oz_XxpYrtFvW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the size of the vocabulary, $V$ is given by:\n",
        "\\begin{align}\n",
        "  V &= |\\{\\texttt{I, am, Sam, do, not, like, green, eggs, and, <s>, </s>}\\}|= 11\n",
        "\\end{align}\n",
        "The add-one smoothed probability can be computed as follows:\n",
        "\\begin{align}\n",
        "  P_{Laplace}(\\texttt{Sam|am}) &= \\frac{C(\\texttt{am Sam})+1}{C(\\texttt{am}) + V} = \\frac{2+1}{3+11} = \\frac{3}{14} \\approx .214\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "vLt2YU39tr3v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.5**: Suppose we didn't use the end-symbol $\\texttt{</s>}$. Train an unsmoothed bigram grammar on the following training corpus without using the end-symbol $\\texttt{</s>}$:\n",
        "\\begin{align}\n",
        "  \\texttt{<s> a b} \\\\\n",
        "  \\texttt{<s> b b} \\\\\n",
        "  \\texttt{<s> b a} \\\\\n",
        "  \\texttt{<s> a a}\n",
        "\\end{align}\n",
        "Demonstrate that your bigram model does not assign a single probability distribution across all sentence lengths by showing that the sum of the probability\n",
        "of the four possible $2$ word sentences over the alphabet $\\texttt{{a,b}}$ is $1.0$, and the sum of the probability of all possible $3$ word sentences over the alphabet $\\texttt{{a,b}}$ is also $1.0$."
      ],
      "metadata": {
        "id": "PPrmUefoHcix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bigram probabilities for the training corpus are as follows:\n",
        "\\begin{align}\n",
        "  P(\\texttt{a|<s>}) &= \\frac{2}{4} = \\frac{1}{2} && &P(\\texttt{a|b}) &= \\frac{1}{2} && &P(\\texttt{a|a}) &= \\frac{1}{2} \\\\\n",
        "  P(\\texttt{b|<s>}) &= \\frac{2}{4} = \\frac{1}{2} && &P(\\texttt{b|a}) &= \\frac{1}{2} && &P(\\texttt{b|b}) &= \\frac{1}{2}\n",
        "\\end{align}\n",
        "Note that due to the lack of end-symbol $\\texttt{</s>}$, the letters at the end of each sentence are not counted in the unigram count for the denominator.\n",
        "\n",
        "Then, we can compute the following joint probabilities:\n",
        "\\begin{align}\n",
        "  P(\\texttt{<s> x y}) &\\approx P(\\texttt{x|<s>})\\cdot P(\\texttt{y|x}) = \\frac{1}{4} && (\\text{for }x, y \\in \\{a, b\\}^2) \n",
        "\\end{align}\n",
        "\n",
        "The sum of the probabilities of the all possible $2$-word sentences over $\\texttt{{a,b}}$ is thus computed as:\n",
        "\\begin{align}\n",
        "  \\sum_{x, y \\in \\{a, b\\}^2}P(\\texttt{<s> x y}) = 4 * \\frac{1}{4} = 1\n",
        "\\end{align}\n",
        "Similarly, we can compute the joint probabilities for $3$-word sentences:\n",
        "\\begin{align}\n",
        "  P(\\texttt{<s> x y z}) &\\approx P(\\texttt{x|<s>})P(\\texttt{y|x})P(\\texttt{z|y}) = \\frac{1}{8} && (\\text{for } x,y,z \\in \\{a, b\\}^{3})\n",
        "\\end{align}\n",
        "Therefore, we have:\n",
        "\\begin{align}\n",
        "  \\sum_{x, y, z \\in \\{a,b\\}^3}P(\\texttt{<s> x y z}) &= 8 * \\frac{1}{8} = 1\n",
        "\\end{align}\n",
        "Therefore, the bigram model does not assign a single proability distribution across all sentence lengths."
      ],
      "metadata": {
        "id": "4kNJ5N_tIJh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.6**: Suppose we train a trigram language model with add-one smoothing on a given corpus. The corpus contains $V$ word types. Express a formula for estimating $P(w_3|w_1 w_2)$ where $w_3$ is a word which follows the bigram $(w_1, w_2)$, in terms of various $n$-gram counts and $V$. Use the notation $c(w_1, w_2, w_3)$ to denote the number of times that trigram $(w_1, w_2, w_3)$ occurs in the corpus, and so on for bigrams and unigrams."
      ],
      "metadata": {
        "id": "QCAjVV-rP9ul"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extending the argument for the bigram model, we have:\n",
        "\\begin{align}\n",
        "  P_{Laplace}(w_3|w_1w_2) &= \\frac{c(w_1,w_2,w_3)+1}{\\sum_{w}\\left(c(w_1,w_2,w)+1\\right)} = \\frac{c(w_1,w_2,w_3)+1}{c(w_1,w_2)+V}\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "aSziYVE9QlZQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.7**: We are given the following corpus, modified from the one in the chapter:\n",
        "\\begin{align}\n",
        "  &\\texttt{<s> I am Sam </s>} \\\\\n",
        "  &\\texttt{<s> Sam I am </s>} \\\\\n",
        "  &\\texttt{<s> I am Sam </s>} \\\\\n",
        "  &\\texttt{<s> I do not like green eggs and Sam </s>} \n",
        "\\end{align}\n",
        "If we use linear interpolation smoothing between a maximum-likelihood bigram model and a maximum-likelihood unigram model with $\\lambda_1 = \\frac{1}{2}$ and $\\lambda_2 = \\frac{1}{2}$, what is $P(\\texttt{Sam|am})$? Include $\\texttt{<s>}$ and $\\texttt{</s>}$ in your counts just like any other token."
      ],
      "metadata": {
        "id": "OjqgJ3E5RxB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Counting from the corpus, we have the following probabilities:\n",
        "\\begin{align}\n",
        "  P(\\texttt{Sam|am}) &= \\frac{2}{3} \\\\\n",
        "  P(\\texttt{Sam}) &= \\frac{4}{25}\n",
        "\\end{align}\n",
        "Using these values, we can compute the linear interpolation smoothing probability:\n",
        "\\begin{align}\n",
        "  \\hat{P}(\\texttt{Sam|am}) &= \\lambda_1 P(\\texttt{Sam|am}) + \\lambda_2 P(\\texttt{Sam}) = \\frac{1}{3} + \\frac{2}{25} \\approx .41\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "JtIHD7O8SeRW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.8**: Write a program to compute unsmoothed unigrams and bigrams."
      ],
      "metadata": {
        "id": "62WSb_E8Sgz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "def process_corpus(corpus, n=2, incl_period=False):\n",
        "  \"\"\"Process corpus by lowercasing everything and inserting special symbols\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  corpus : str\n",
        "    corpus text where sentences are\n",
        "    separated by periods.\n",
        "  incl_period : bool\n",
        "    whether to include period in unigram counts\n",
        "    only applies to n == 1 case\n",
        "  n : int\n",
        "    parameter n of n-gram approximation\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  list\n",
        "    list of lower-cased sentences with special symbols inserted\n",
        "  \"\"\"\n",
        "  if n == 1:\n",
        "    result = []\n",
        "    curr_str = \"\"\n",
        "    for c in corpus:\n",
        "      if c.isalnum():\n",
        "        curr_str += c\n",
        "      else:\n",
        "        if curr_str:\n",
        "          result.append(curr_str.lower())\n",
        "        if incl_period and c == \".\":\n",
        "          result.append(c)\n",
        "        curr_str = \"\"\n",
        "    if curr_str:\n",
        "      result.append(curr_str.lower())\n",
        "    return result\n",
        "  \n",
        "  result = []\n",
        "  tmp_ls = [\"<s>\"]*(n-1)\n",
        "  tmp_str = \"\"\n",
        "  for c in corpus:\n",
        "    if c.isalnum():\n",
        "      tmp_str += c\n",
        "    else:\n",
        "      if tmp_str:\n",
        "        tmp_ls.append(tmp_str.lower())\n",
        "      if c == '.':\n",
        "        result.append(tmp_ls + [\"</s>\"]*(n-1))\n",
        "        tmp_ls = [\"<s>\"]*(n-1)\n",
        "      tmp_str = \"\"\n",
        "\n",
        "  # If final sentence does not end with a period, add to list\n",
        "  if tmp_str:\n",
        "    tmp_ls.append(tmp_str.lower())\n",
        "  if tmp_ls != [\"<s>\"]*(n-1):\n",
        "    result.append(tmp_ls + [\"</s>\"]*(n-1))\n",
        "  return result\n",
        "\n",
        "def ngram(processed_corpus, words, n=2):\n",
        "  \"\"\"Compute ngram of curr_word given prev_words trained on corpus.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  processed_corpus : list\n",
        "    processed corpus text\n",
        "  words : list\n",
        "    list of words to compute ngram probability\n",
        "  n : int\n",
        "    parameter n of n-gram approximation\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  double\n",
        "    the ngram probability P(words)\n",
        "  \"\"\"\n",
        "  # lowercase the words:\n",
        "  words = [word.lower() for word in words]\n",
        "  # Raise error if length of words < n\n",
        "  if len(words) < n:\n",
        "    print(\"Please provide list of words of adequate size.\")\n",
        "    return\n",
        "  #ngram counts\n",
        "  counter_n = Counter()\n",
        "  for sentence in processed_corpus:\n",
        "    for i in range(len(sentence)-n+1):\n",
        "      counter_n[tuple(sentence[i:i+n])] += 1\n",
        "  #(n-1)gram counts (only if n > 1)\n",
        "  counter_n1 = Counter()\n",
        "  if n > 1:\n",
        "    for sentence in processed_corpus:\n",
        "      for i in range(len(sentence)-n+2):\n",
        "        counter_n1[tuple(sentence[i:i+n-1])] += 1\n",
        "\n",
        "  # Compute log of ngram conditional probability\n",
        "  def ngram_log_prob(counter_n, counter_n1, curr_word, prev_words):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    counter_n : Counter\n",
        "      ngram Counter of corpus\n",
        "    counter_n1 : Counter\n",
        "      (n-1)gram Counter of corpus\n",
        "      None if n == 1\n",
        "    prev_words : list\n",
        "      previous words to condition on\n",
        "      empty if n == 1\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    double\n",
        "      the log10 of ngram conditional probability P(curr_word|prev_words)\n",
        "    \"\"\"\n",
        "    ngram_phrase = tuple([w for w in prev_words] + [curr_word])\n",
        "    # if phrase is not in corpus, return neg infinity\n",
        "    if ngram_phrase not in counter_n:\n",
        "      return float('-inf')\n",
        "    # if unigram, return simple probability value\n",
        "    if not prev_words:\n",
        "      return math.log10((counter_n[tuple(curr_word)])/(sum(counter_n.values())))\n",
        "    return math.log10((counter_n[ngram_phrase])/(counter_n1[tuple(prev_words)]))\n",
        "  \n",
        "  log_result = 0\n",
        "  for i in range(len(words)-n+1):\n",
        "    log_result += ngram_log_prob(counter_n, counter_n1, words[i+n-1], words[i:i+n-1])\n",
        "  return 10**log_result"
      ],
      "metadata": {
        "id": "kD2I8vzfVJE5"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check\n",
        "corpus = \"I am Sam. Sam I am. I do not like green eggs and ham.\"\n",
        "assert ngram(process_corpus(corpus, 2), ['I', 'am'], 2) == 2/3\n",
        "assert ngram(process_corpus(corpus, 2), ['am', 'Sam'], 2) == 1/2\n",
        "print(\"Assertion passed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gq_zxnHYrNYE",
        "outputId": "42900319-5e2b-4c5b-cb32-c26f2f56642b"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assertion passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.9**: Run your $n$-gram program on two different small corpora of your choice (you might use email text or newsgroups). Now compare the statistics of the two\n",
        "corpora. What are the differences in the most common unigrams between the two? How about interesting differences in bigrams?"
      ],
      "metadata": {
        "id": "GQZIGvJfSlge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def most_common_ngram(corpus, n=1, k=1):\n",
        "  \"\"\"Compute k most common ngrams of corpus.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  corpus : str\n",
        "    corpus text\n",
        "  n : int\n",
        "    parameter n of ngram\n",
        "  k : int\n",
        "    number of most common unigrams to return\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  \n",
        "  list\n",
        "    list of tuples(ngram, ngram count) for\n",
        "    k most common ngrams\n",
        "  \"\"\"\n",
        "  proc_corpus = process_corpus(corpus, n)\n",
        "  if n == 1:\n",
        "    c = Counter(proc_corpus)\n",
        "  else:\n",
        "    c = Counter()\n",
        "    for sentence in proc_corpus:\n",
        "      for i in range(len(sentence)-n+1):\n",
        "        c[tuple(sentence[i:i+n])] += 1\n",
        "  return c.most_common()[:k]"
      ],
      "metadata": {
        "id": "PBe40nI9z_iG"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "from nltk.corpus import gutenberg\n",
        "import pprint\n",
        "\n",
        "# Run ngram on Emma (Jane Austen) and Macbeth (Shakespeare)\n",
        "emma_raw, macbeth_raw = gutenberg.raw('austen-emma.txt'), gutenberg.raw('shakespeare-macbeth.txt')\n",
        "\n",
        "print(most_common_ngram(emma_raw, 2, 5))\n",
        "print(most_common_ngram(macbeth_raw, 2, 5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ux0KCDDSvt2K",
        "outputId": "f5de7188-e77d-4e23-8135-ffba5d67febe"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "['emma', 'by', 'jane', 'austen', '1816', 'volume', 'i', 'chapter', 'i', 'emma', 'woodhouse', 'handsome', 'clever', 'and', 'rich', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', 'and', 'had', 'lived', 'nearly', 'twenty', 'one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', '.', 'she', 'was', 'the', 'youngest', 'of', 'the', 'two', 'daughters', 'of', 'a', 'most', 'affectionate', 'indulgent', 'father', 'and', 'had', 'in', 'consequence', 'of', 'her', 'sister', 's', 'marriage', 'been', 'mistress', 'of', 'his', 'house', 'from', 'a', 'very', 'early', 'period', '.', 'her', 'mother', 'had', 'died', 'too', 'long', 'ago', 'for', 'her', 'to', 'have', 'more', 'than', 'an', 'indistinct', 'remembrance', 'of', 'her', 'caresses', 'and', 'her', 'place', 'had', 'been', 'supplied', 'by', 'an', 'excellent', 'woman', 'as', 'governess', 'who', 'had', 'fallen', 'little', 'short', 'of', 'a', 'mother', 'in', 'affection', '.', 'sixteen', 'years', 'had', 'miss', 'taylor', 'been', 'in', 'mr', '.', 'woodhouse', 's', 'family', 'less', 'as', 'a', 'governess', 'than', 'a', 'friend', 'very', 'fond', 'of', 'both', 'daughters', 'but', 'particularly', 'of', 'emma', '.', 'between', 'them', 'it', 'was', 'more', 'the', 'intimacy', 'of', 'sisters', '.', 'even', 'before', 'miss', 'taylor', 'had', 'ceased', 'to', 'hold', 'the', 'nominal', 'office', 'of', 'governess', 'the', 'mildness', 'o']\n",
            "[(('mr', '</s>'), 1154), (('<s>', 'i'), 853), (('mrs', '</s>'), 701), (('to', 'be'), 608), (('of', 'the'), 561)]\n",
            "[(('macb', '</s>'), 137), (('<s>', 'enter'), 73), (('<s>', 'i'), 70), (('macd', '</s>'), 58), (('<s>', 'what'), 50)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.10**: Add an option to your program to generate random sentences."
      ],
      "metadata": {
        "id": "OoKsGH6BSuyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "def unigram_generate_sentences(corpus, num_sent):\n",
        "  \"\"\"Generate random sentences trained on corpus\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  corpus : str\n",
        "    corpus text\n",
        "  num_sent: int\n",
        "    number of sentences to generate\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  text\n",
        "    sentences generated according to unigram probability distribution\n",
        "  \"\"\"\n",
        "  proc_corpus = process_corpus(corpus, 1, True)\n",
        "  curr_sent, capital, res = 0, True, \"\"\n",
        "  c = Counter(proc_corpus)\n",
        "  total = sum(c.values())\n",
        "  prob_dist = [c[word]/total for word in c]\n",
        "  while curr_sent < num_sent:\n",
        "    curr = np.random.choice(list(c.keys()), p=prob_dist)\n",
        "    if curr == \".\":\n",
        "      if capital:\n",
        "        continue\n",
        "      else:\n",
        "        curr_sent += 1\n",
        "        capital = True\n",
        "        res = res[:-1] + \". \"\n",
        "    else:\n",
        "      if capital:\n",
        "        curr = curr[0].upper() + curr[1:]\n",
        "      res += curr + \" \"\n",
        "      capital = False\n",
        "  return res\n",
        "\n",
        "def ngram_generate_sentences(corpus, num_sent, n=2):\n",
        "  \"\"\"Generate random sentences trained on corpus\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  corpus : str\n",
        "    corpus text\n",
        "  num_sent: int\n",
        "    number of sentences to generate\n",
        "  n : int\n",
        "    parameter n of ngram\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  text\n",
        "    sentences generated according to ngram probability distribution\n",
        "  \"\"\"\n",
        "  if n == 1:\n",
        "    return unigram_generate_sentences(corpus, num_sent)\n",
        "\n",
        "  proc_corpus = process_corpus(corpus, n)\n",
        "  res, ngram_prob = [], {}\n",
        "  # ngram, (n-1)gram counts\n",
        "  n1_counter, n_counter = Counter(), Counter()\n",
        "  for sentence in proc_corpus:\n",
        "    for i in range(len(sentence)-n+2):\n",
        "      if i < len(sentence)-n+1:\n",
        "        n_counter[tuple(sentence[i:i+n])] += 1\n",
        "      n1_counter[tuple(sentence[i:i+n-1])] += 1\n",
        "  # Compute ngram probabilities for all existing n-tuples in corpus\n",
        "  for sentence in proc_corpus:\n",
        "    for i in range(len(sentence)-n+1):\n",
        "      ngram_prob[tuple(sentence[i:i+n])] = n_counter[tuple(sentence[i:i+n])]/n1_counter[tuple(sentence[i:i+n-1])]\n",
        "  # Generate sentences\n",
        "  curr_sent = 0\n",
        "  prev, res, capital = (\"<s>\",)*(n-1), \"\", True\n",
        "  while curr_sent < num_sent:\n",
        "    choices = [p for p in list(ngram_prob.keys()) if p[:n-1] == prev]\n",
        "    ind = [i for i in range(len(choices))]\n",
        "    dist = [ngram_prob[choice] for choice in choices]\n",
        "    curr = choices[np.random.choice(ind, p=dist)]\n",
        "    if curr[-1] == \"</s>\":\n",
        "      res += \".\"\n",
        "      capital = True\n",
        "      prev = (\"<s>\",)*(n-1)\n",
        "      curr_sent += 1\n",
        "    else:\n",
        "      curr_text = \"\"\n",
        "      if capital:\n",
        "        curr_text = \" \".join(curr[n-1:])\n",
        "        curr_text = curr_text[0].upper() + curr_text[1:]\n",
        "      else:\n",
        "        curr_text = curr[-1]\n",
        "      res += \" \" +curr_text\n",
        "      capital = False\n",
        "      prev = curr[1:]\n",
        "  return res\n"
      ],
      "metadata": {
        "id": "JOGJhiRv6Byl"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "from nltk.corpus import gutenberg\n",
        "import pprint\n",
        "\n",
        "# Run ngram on Emma (Jane Austen) and Macbeth (Shakespeare)\n",
        "emma_raw, macbeth_raw = gutenberg.raw('austen-emma.txt'), gutenberg.raw('shakespeare-macbeth.txt')\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "\n",
        "pp.pprint(ngram_generate_sentences(emma_raw, 10, n=4))"
      ],
      "metadata": {
        "id": "Ne63kzUiJuQl",
        "outputId": "1e310eea-b22a-4611-8b66-12a31aef0250",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "(' The decree is issued by somebody. Churchill lived i suppose there is not '\n",
            " 'such another nursery establishment so liberal and so fond of each other. '\n",
            " 'Weston was afraid of draughts for the young people will find a way. Weston s '\n",
            " 'arguments against herself. Knightley would ask but a very short parley with '\n",
            " 'her own sheets an excellent precaution. You knew i should not wonder if it '\n",
            " 'were that it would be really having frank in their neighbourhood. Two such '\n",
            " 'could never be granted. The affection of sixteen years how she had played '\n",
            " 'with her from five years old how she had taught and how she had devoted all '\n",
            " 'her powers to attach and amuse her in health and spirits for going that they '\n",
            " 'made a point of some dignity as well as miss smith s conversation but she '\n",
            " 'found her decidedly more sensible than before of mr. Perhaps i intended you '\n",
            " 'to say and look every thing that look and manner could do made emma feel '\n",
            " 'that she had never been quite well since the time of year better weather and '\n",
            " 'that he knew how to do from the consciousness of there being no present '\n",
            " 'danger in returning home but no assurances could convince him that they '\n",
            " 'might in one of the sort of constant expectation there will be very bad. '\n",
            " 'Robert martin.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "from nltk.corpus import gutenberg\n",
        "import pprint\n",
        "\n",
        "# Run ngram on Emma (Jane Austen) and Macbeth (Shakespeare)\n",
        "emma_raw, macbeth_raw = gutenberg.raw('austen-emma.txt'), gutenberg.raw('shakespeare-macbeth.txt')\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "mac, emma, uni, bi = \"MACBETH\", \"EMMA\", \"UNIGRAM SENTENCES\", \"BIGRAM SENTENCES\"\n",
        "mac_uni, mac_bi = \" \".join([mac, uni]), \" \".join([mac, bi])\n",
        "emma_uni, emma_bi = \" \".join([emma, uni]), \" \".join([emma, bi])\n",
        "# Macbeth unigram-generated sentences:\n",
        "print('\\n')\n",
        "print(\"=\"*int((80-len(mac_uni))/2) + mac_uni + \"=\"*int((80-len(mac_uni))/2))\n",
        "pp.pprint(unigram_generate_sentences(macbeth_raw, 4))\n",
        "# Emma unigram-generated sentences:\n",
        "print('\\n')\n",
        "print(\"=\"*int((80-len(emma_uni))/2) + emma_uni + \"=\"*int((80-len(emma_uni))/2))\n",
        "pp.pprint(unigram_generate_sentences(emma_raw, 4))\n",
        "# Macbeth bigram-generated sentences:\n",
        "print('\\n')\n",
        "print(\"=\"*int((80-len(mac_bi))/2) + mac_bi + \"=\"*int((80-len(mac_bi))/2))\n",
        "# Emma bigram-generated sentences\n",
        "print('\\n')\n",
        "print(\"=\"*int((80-len(emma_bi))/2) + emma_bi + \"=\"*int((80-len(emma_bi))/2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Bh03JUvBq4w",
        "outputId": "d07316c5-628d-4f4e-9d79-970f025850f2"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "\n",
            "\n",
            "===========================MACBETH UNIGRAM SENTENCES===========================\n",
            "('None. Haue be and o captiuitie thou each shew may which and i my it macbeth '\n",
            " 'such a this neuer within gent st it set my. Now dispaire noyse. Is of life '\n",
            " 'not good tongue against repeat bathe vessell about worth the deuils him the '\n",
            " 'equiuocation and 2 not much is t i. ')\n",
            "\n",
            "\n",
            "=============================EMMA UNIGRAM SENTENCES=============================\n",
            "('Pianoforte. Mrs. Case these be parlours have her you my answer my little of '\n",
            " 'voice shall lasts not was who related of naturally two the. Must without '\n",
            " 'allow emma how churchill perhaps friends would month back speak the amends '\n",
            " 'many what to thrown a she s to not mind should two aloud suspiciously was '\n",
            " 'sort wife without can always woman very not. ')\n",
            "\n",
            "\n",
            "============================MACBETH BIGRAM SENTENCES============================\n",
            "\n",
            "\n",
            "=============================EMMA BIGRAM SENTENCES=============================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.11**: Add an option to your program to compute the perplexity of a test set."
      ],
      "metadata": {
        "id": "pk0F1Ti5S0Ew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.12**: You are given a training set of $100$ numbers that consists of $91$ zeros and $1$ each of the other digits $1-9$. Now we see the following test set: 0 0 0 0 0 3 0 0 0 0. What is the unigram perplexity?"
      ],
      "metadata": {
        "id": "QlgkusZqNayA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model trained using the skewed training set learns the following probabilities:\n",
        "\\begin{align}\n",
        "  P(0) &= .91 && &P(1) &= P(2) = \\dots = P(9) = .01\n",
        "\\end{align}\n",
        "\n",
        "Therefore, the unigram perplexity of the test set can be computed as follows:\n",
        "\\begin{align}\n",
        "  PP(W) &= P(0000030000)^{-\\frac{1}{10}} = \\left(P(0)^{9}P(3)\\right)^{-\\frac{1}{10}} \\approx 1.725\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "WqP4njuVNuJs"
      }
    }
  ]
}