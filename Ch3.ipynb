{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peterchang0414/SLP/blob/main/Ch3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3: N-gram Language Models"
      ],
      "metadata": {
        "id": "qk47KCC8e3kD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solution by: Peter Chang (GItHub: @peterchang0414)\n",
        "\n",
        "*Last edited: 2021.12.22*"
      ],
      "metadata": {
        "id": "6A8G9Nch76Cq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1**: Write out the equation for trigram probability estimation (modifying Eq. 3.11). Now write out all the non-zero trigram probabilities for the *I am Sam* corpus on page 32."
      ],
      "metadata": {
        "id": "bxhs_F7yfDEK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a trigram model, we make the following approximation:\n",
        "\\begin{align}\n",
        "        P(w_n|w_{1:n-1}) \\approx P(w_n|w_{n-2}w_{n-1})\n",
        "\\end{align}\n",
        "As with the bigram model, to compute the trigram probability of a word $z$ given previous two words $x$ and $y$, we compute the count of the trigram $C(xyz)$ and normalize by ubigram count of $C(xy)$:\n",
        "\\begin{align}\n",
        "  P(w_n|w_{n-2}w_{n-1}) &= \\frac{C(w_{n-2}w_{n-1}w_{n})}{C(w_{n-2}w_{n-1})}\n",
        "\\end{align}\n",
        "\n",
        "For the *I am Sam* corpus, we augment each sentence with two specials at the beginning and at the end:\n",
        "\n",
        "\\begin{align}\n",
        "  &\\texttt{<s><s> I am Sam </s></s>} \\\\\n",
        "  &\\texttt{<s><s> Sam I am </s></s>} \\\\\n",
        "  &\\texttt{<s><s> I do not like green eggs and ham </s></s>}\n",
        "\\end{align}\n",
        "\n",
        "Then, the following are all the non-zero trigram probabilities for the corpus:\n",
        "\n",
        "\\begin{align}\n",
        "  P(\\texttt{I}|\\texttt{<s><s>}) &= \\frac{2}{3} = .67, && &P(\\texttt{Sam}|\\texttt{<s><s>}) &= \\frac{1}{3} = .33 && &P(\\texttt{am}|\\texttt{<s> I}) &= \\frac{1}{2}=.5 \\\\\n",
        "  P(\\texttt{I}|\\texttt{<s> Sam}) &= 1 && &P(\\texttt{do}|\\texttt{<s> I}) &= \\frac{1}{2} = .5 && &P(\\texttt{Sam}|\\texttt{I am}) &= \\frac{1}{2} = .5 \\\\\n",
        "  P(\\texttt{am}|\\texttt{Sam I}) &= 1 && &P(\\texttt{not}|\\texttt{I do}) &= 1 && &P(\\texttt{</s>}|\\texttt{am Sam}) &= 1 \\\\\n",
        "  P(\\texttt{</s>}|\\texttt{I am}) &= \\frac{1}{2} = .5 && &P(\\texttt{like}|\\texttt{do not}) &= 1 && &P(\\texttt{</s>}|\\texttt{Sam </s>}) &= 1 \\\\\n",
        "  P(\\texttt{</s>}|\\texttt{am </s>}) &= 1 && &P(\\texttt{green}|\\texttt{not like}) &= 1 && &P(\\texttt{eggs}|\\texttt{like green}) &= 1 \\\\\n",
        "  P(\\texttt{and}|\\texttt{green eggs}) &= 1 && &P(\\texttt{ham}|\\texttt{eggs and}) &= 1 && &P(\\texttt{</s>}|\\texttt{and ham}) &= 1 \\\\\n",
        "  P(\\texttt{</s>}|\\texttt{ham </s>}) &= 1\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "vkrdKv5_-vzM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.2**: Calculate the probability of the sentence $\\texttt{i want chinese food}$. Give two probabilities, one using Fig. 3.2 and the 'useful probabililties' just below it on page 34, and another using the add-1 smoothed table in Fig. 3.7. Assume the additional add-1 smoothed probabilities $P(\\texttt{i}|\\texttt{<s>}) = 0.19$ and $P(\\texttt{</s>}|\\texttt{food}) = 0.40$."
      ],
      "metadata": {
        "id": "Kg0BkgRuAMsz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Fig 3.2, we can compute the probability of the sentence as follows:\n",
        "\\begin{align}\n",
        "  P(\\texttt{<s> i want chinese food </s>}) &= P(\\texttt{i|<s>})\\cdot P(\\texttt{want|i}) \\cdot P(\\texttt{chinese|want}) \\cdot P(\\texttt{food|chinese}) \\cdot P(\\texttt{</s>|food}) \\\\\n",
        "  &= .25 \\times .33 \\times .0065 \\times .52 \\times .68 \\\\\n",
        "  &\\approx .00019\n",
        "\\end{align}\n",
        "\n",
        "Using the add-1 smoothed probabilities, we can compute the probability of the sentence as follows:\n",
        "\\begin{align}\n",
        "  P(\\texttt{<s> i want chinese food </s>}) &= P(\\texttt{i|<s>})\\cdot P(\\texttt{want|i}) \\cdot P(\\texttt{chinese|want}) \\cdot P(\\texttt{food|chinese}) \\cdot P(\\texttt{</s>|food}) \\\\\n",
        "  &= .19 \\times .21 \\times .0029 \\times .052 \\times .40 \\\\\n",
        "  &\\approx .0000024\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "s6tl1RiTCQg2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.3**: Which of the two probabilities your computed in the previous exercise is higher, unsmoothed or smoothed? Explain why."
      ],
      "metadata": {
        "id": "7ndt4KlSsmhy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The unsmoothed probability is higher than the smoothed probability. This is because all the bigram word pairs in the sentence had non-zero probability in the unsmoothed model, which means the smoothing operation took away probabilities from those pairs to add to the pairs which initially have probabilities of zero."
      ],
      "metadata": {
        "id": "upQfauyNsuED"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.4**: We are given the following corpus, modified from the one in the chapter:\n",
        "\\begin{align}\n",
        "  &\\texttt{<s> I am Sam </s>} \\\\\n",
        "  &\\texttt{<s> Sam I am </s>} \\\\\n",
        "  &\\texttt{<s> I am Sam </s>} \\\\\n",
        "  &\\texttt{<s> I do not like green eggs and Sam </s>} \n",
        "\\end{align}\n",
        "Using a bigram language model with add-one smoothing, what is $P(\\texttt{Sam|am})$? Include $\\texttt{<s>}$ and $\\texttt{</s>}$ in your counts just like any other token."
      ],
      "metadata": {
        "id": "oz_XxpYrtFvW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the size of the vocabulary, $V$ is given by:\n",
        "\\begin{align}\n",
        "  V &= |\\{\\texttt{I, am, Sam, do, not, like, green, eggs, and, <s>, </s>}\\}|= 11\n",
        "\\end{align}\n",
        "The add-one smoothed probability can be computed as follows:\n",
        "\\begin{align}\n",
        "  P_{Laplace}(\\texttt{Sam|am}) &= \\frac{C(\\texttt{am Sam})+1}{C(\\texttt{am}) + V} = \\frac{2+1}{3+11} = \\frac{3}{14} \\approx .214\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "vLt2YU39tr3v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.5**: Suppose we didn't use the end-symbol $\\texttt{</s>}$. Train an unsmoothed bigram grammar on the following training corpus without using the end-symbol $\\texttt{</s>}$:\n",
        "\\begin{align}\n",
        "  \\texttt{<s> a b} \\\\\n",
        "  \\texttt{<s> b b} \\\\\n",
        "  \\texttt{<s> b a} \\\\\n",
        "  \\texttt{<s> a a}\n",
        "\\end{align}\n",
        "Demonstrate that your bigram model does not assign a single probability distribution across all sentence lengths by showing that the sum of the probability\n",
        "of the four possible $2$ word sentences over the alphabet $\\texttt{{a,b}}$ is $1.0$, and the sum of the probability of all possible $3$ word sentences over the alphabet $\\texttt{{a,b}}$ is also $1.0$."
      ],
      "metadata": {
        "id": "PPrmUefoHcix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bigram probabilities for the training corpus are as follows:\n",
        "\\begin{align}\n",
        "  P(\\texttt{a|<s>}) &= \\frac{2}{4} = \\frac{1}{2} && &P(\\texttt{a|b}) &= \\frac{1}{2} && &P(\\texttt{a|a}) &= \\frac{1}{2} \\\\\n",
        "  P(\\texttt{b|<s>}) &= \\frac{2}{4} = \\frac{1}{2} && &P(\\texttt{b|a}) &= \\frac{1}{2} && &P(\\texttt{b|b}) &= \\frac{1}{2}\n",
        "\\end{align}\n",
        "Note that due to the lack of end-symbol $\\texttt{</s>}$, the letters at the end of each sentence are not counted in the unigram count for the denominator.\n",
        "\n",
        "Then, we can compute the following joint probabilities:\n",
        "\\begin{align}\n",
        "  P(\\texttt{<s> x y}) &\\approx P(\\texttt{x|<s>})\\cdot P(\\texttt{y|x}) = \\frac{1}{4} && (\\text{for }x, y \\in \\{a, b\\}^2) \n",
        "\\end{align}\n",
        "\n",
        "The sum of the probabilities of the all possible $2$-word sentences over $\\texttt{{a,b}}$ is thus computed as:\n",
        "\\begin{align}\n",
        "  \\sum_{x, y \\in \\{a, b\\}^2}P(\\texttt{<s> x y}) = 4 * \\frac{1}{4} = 1\n",
        "\\end{align}\n",
        "Similarly, we can compute the joint probabilities for $3$-word sentences:\n",
        "\\begin{align}\n",
        "  P(\\texttt{<s> x y z}) &\\approx P(\\texttt{x|<s>})P(\\texttt{y|x})P(\\texttt{z|y}) = \\frac{1}{8} && (\\text{for } x,y,z \\in \\{a, b\\}^{3})\n",
        "\\end{align}\n",
        "Therefore, we have:\n",
        "\\begin{align}\n",
        "  \\sum_{x, y, z \\in \\{a,b\\}^3}P(\\texttt{<s> x y z}) &= 8 * \\frac{1}{8} = 1\n",
        "\\end{align}\n",
        "Therefore, the bigram model does not assign a single proability distribution across all sentence lengths."
      ],
      "metadata": {
        "id": "4kNJ5N_tIJh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.6**: Suppose we train a trigram language model with add-one smoothing on a given corpus. The corpus contains $V$ word types. Express a formula for estimating $P(w_3|w_1 w_2)$ where $w_3$ is a word which follows the bigram $(w_1, w_2)$, in terms of various $n$-gram counts and $V$. Use the notation $c(w_1, w_2, w_3)$ to denote the number of times that trigram $(w_1, w_2, w_3)$ occurs in the corpus, and so on for bigrams and unigrams."
      ],
      "metadata": {
        "id": "QCAjVV-rP9ul"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extending the argument for the bigram model, we have:\n",
        "\\begin{align}\n",
        "  P_{Laplace}(w_3|w_1w_2) &= \\frac{c(w_1,w_2,w_3)+1}{\\sum_{w}\\left(c(w_1,w_2,w)+1\\right)} = \\frac{c(w_1,w_2,w_3)+1}{c(w_1,w_2)+V}\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "aSziYVE9QlZQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.7**: We are given the following corpus, modified from the one in the chapter:\n",
        "\\begin{align}\n",
        "  &\\texttt{<s> I am Sam </s>} \\\\\n",
        "  &\\texttt{<s> Sam I am </s>} \\\\\n",
        "  &\\texttt{<s> I am Sam </s>} \\\\\n",
        "  &\\texttt{<s> I do not like green eggs and Sam </s>} \n",
        "\\end{align}\n",
        "If we use linear interpolation smoothing between a maximum-likelihood bigram model and a maximum-likelihood unigram model with $\\lambda_1 = \\frac{1}{2}$ and $\\lambda_2 = \\frac{1}{2}$, what is $P(\\texttt{Sam|am})$? Include $\\texttt{<s>}$ and $\\texttt{</s>}$ in your counts just like any other token."
      ],
      "metadata": {
        "id": "OjqgJ3E5RxB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Counting from the corpus, we have the following probabilities:\n",
        "\\begin{align}\n",
        "  P(\\texttt{Sam|am}) &= \\frac{2}{3} \\\\\n",
        "  P(\\texttt{Sam}) &= \\frac{4}{25}\n",
        "\\end{align}\n",
        "Using these values, we can compute the linear interpolation smoothing probability:\n",
        "\\begin{align}\n",
        "  \\hat{P}(\\texttt{Sam|am}) &= \\lambda_1 P(\\texttt{Sam|am}) + \\lambda_2 P(\\texttt{Sam}) = \\frac{1}{3} + \\frac{2}{25} \\approx .41\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "JtIHD7O8SeRW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.8**: Write a program to compute unsmoothed unigrams and bigrams."
      ],
      "metadata": {
        "id": "62WSb_E8Sgz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "from nltk.corpus import gutenberg\n",
        "import pprint\n",
        "import math\n",
        "\n",
        "def process_corpus(corpus, n=2):\n",
        "  \"\"\"Process corpus.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  corpus : str\n",
        "    corpus text where sentences are\n",
        "    separated by periods.\n",
        "  n : int\n",
        "    parameter n of n-gram approximation\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  list\n",
        "    list of sentences with special symbols inserted\n",
        "  \"\"\"\n",
        "  if n == 1:\n",
        "    return list(filter(None, re.split(r'[^a-zA-Z1-9]', corpus)))\n",
        "  \n",
        "  result = []\n",
        "  tmp_ls = [\"<s>\"]*(n-1)\n",
        "  tmp_str = \"\"\n",
        "  for c in corpus:\n",
        "    if c == '.':\n",
        "      result.append(tmp_ls + [\"</s>\"]*(n-1))\n",
        "      tmp_ls, tmp_str = [\"<s>\"]*(n-1), \"\"\n",
        "    elif c.isalnum():\n",
        "      tmp_str += c\n",
        "    else:\n",
        "      if tmp_str:\n",
        "        tmp_ls.append(tmp_str)\n",
        "      tmp_str = \"\"\n",
        "\n",
        "  # If final sentence does not end with a period, add to list\n",
        "  if tmp_str:\n",
        "    tmp_ls.append(tmp_str)\n",
        "  if tmp_ls != [\"<s>\"]*(n-1):\n",
        "    result.append(tmp_ls + [\"</s>\"]*(n-1))\n",
        "  return result\n",
        "\n",
        "def ngram(corpus, words, n=2):\n",
        "  \"\"\"Compute ngram of curr_word given prev_words trained on corpus.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  corpus : str\n",
        "    corpus text where sentences are\n",
        "    separated by periods.\n",
        "  words : list\n",
        "    list of words to compute ngram probability\n",
        "  n : int\n",
        "    parameter n of n-gram approximation\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  double\n",
        "    the ngram probability P(words)\n",
        "  \"\"\"\n",
        "  processed_corpus = process_corpus(corpus, n)\n",
        "  # Raise error if length of words < n\n",
        "  if len(words) < n:\n",
        "    print(\"Please provide list of words of adequate size.\")\n",
        "    return\n",
        "  log_result = 0\n",
        "  #ngram counts\n",
        "  counter_n = Counter()\n",
        "  for sentence in processed_corpus:\n",
        "    for i in range(len(sentence)-n+1):\n",
        "      counter_n[tuple(sentence[i:i+n])] += 1\n",
        "  #(n-1)gram counts (only if n > 1)\n",
        "  counter_n1 = Counter()\n",
        "  if n > 1:\n",
        "    for sentence in processed_corpus:\n",
        "      for i in range(len(sentence)-n+2):\n",
        "        counter_n1[tuple(sentence[i:i+n-1])] += 1\n",
        "  print(counter_n)\n",
        "  print(counter_n1)\n",
        "\n",
        "  # Compute log of ngram conditional probability\n",
        "  def ngram_log_prob(counter_n, counter_n1, curr_word, prev_words):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    counter_n : Counter\n",
        "      ngram Counter of corpus\n",
        "    counter_n1 : Counter\n",
        "      (n-1)gram Counter of corpus\n",
        "      None if n == 1\n",
        "    prev_words : list\n",
        "      previous words to condition on\n",
        "      empty if n == 1\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    double\n",
        "      the log10 of ngram conditional probability P(curr_word|prev_words)\n",
        "    \"\"\"\n",
        "    ngram_phrase = tuple([w for w in prev_words] + [curr_word])\n",
        "    # if phrase is not in corpus, return neg infinity\n",
        "    if ngram_phrase not in counter_n:\n",
        "      return float('-inf')\n",
        "    # if unigram, return simple probability value\n",
        "    if not prev_words:\n",
        "      return math.log10((counter_n[curr_word])/(counter_n.total()))\n",
        "    return math.log10((counter_n[ngram_phrase])/(counter_n1[tuple(prev_words)]))\n",
        "\n",
        "  for i in range(len(words)-n+1):\n",
        "    log_result += ngram_log_prob(counter_n, counter_n1, words[i+n-1], words[i:i+n-1])\n",
        "  return math.exp(log_result)\n",
        "\n",
        "\n",
        "\n",
        "emma_raw = gutenberg.raw('austen-emma.txt')\n",
        "emma_abridged = emma_raw[:1301]\n",
        "print(ngram(emma_abridged, ['She', 'was', 'the'], 2))\n",
        "# pp = pprint.PrettyPrinter(indent=4, width=50, compact=True)\n",
        "# pp.pprint(process_corpus(emma_abridged, 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kD2I8vzfVJE5",
        "outputId": "c0f3b1a5-0db1-4b79-af5d-8631b70bfe89"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "Counter({('of', 'her'): 3, ('Miss', 'Taylor'): 3, ('of', 'the'): 2, ('and', 'had'): 2, ('of', 'a'): 2, ('her', 'to'): 2, ('had', 'been'): 2, ('in', '</s>'): 2, ('friend', 'very'): 2, ('of', '</s>'): 2, ('<s>', 'Emma'): 1, ('Emma', 'by'): 1, ('by', 'Jane'): 1, ('Jane', 'Austen'): 1, ('Austen', '1816'): 1, ('1816', 'VOLUME'): 1, ('VOLUME', 'I'): 1, ('I', 'CHAPTER'): 1, ('CHAPTER', 'I'): 1, ('I', 'Emma'): 1, ('Emma', 'Woodhouse'): 1, ('Woodhouse', 'handsome'): 1, ('handsome', 'clever'): 1, ('clever', 'and'): 1, ('and', 'rich'): 1, ('rich', 'with'): 1, ('with', 'a'): 1, ('a', 'comfortable'): 1, ('comfortable', 'home'): 1, ('home', 'and'): 1, ('and', 'happy'): 1, ('happy', 'disposition'): 1, ('disposition', 'seemed'): 1, ('seemed', 'to'): 1, ('to', 'unite'): 1, ('unite', 'some'): 1, ('some', 'of'): 1, ('the', 'best'): 1, ('best', 'blessings'): 1, ('blessings', 'of'): 1, ('of', 'existence'): 1, ('existence', 'and'): 1, ('had', 'lived'): 1, ('lived', 'nearly'): 1, ('nearly', 'twenty'): 1, ('twenty', 'one'): 1, ('one', 'years'): 1, ('years', 'in'): 1, ('in', 'the'): 1, ('the', 'world'): 1, ('world', 'with'): 1, ('with', 'very'): 1, ('very', 'little'): 1, ('little', 'to'): 1, ('to', 'distress'): 1, ('distress', 'or'): 1, ('or', 'vex'): 1, ('vex', '</s>'): 1, ('<s>', 'She'): 1, ('She', 'was'): 1, ('was', 'the'): 1, ('the', 'youngest'): 1, ('youngest', 'of'): 1, ('the', 'two'): 1, ('two', 'daughters'): 1, ('daughters', 'of'): 1, ('a', 'most'): 1, ('most', 'affectionate'): 1, ('affectionate', 'indulgent'): 1, ('indulgent', 'father'): 1, ('father', 'and'): 1, ('had', 'in'): 1, ('in', 'consequence'): 1, ('consequence', 'of'): 1, ('her', 'sister'): 1, ('sister', 's'): 1, ('s', 'marriage'): 1, ('marriage', 'been'): 1, ('been', 'mistress'): 1, ('mistress', 'of'): 1, ('of', 'his'): 1, ('his', 'house'): 1, ('house', 'from'): 1, ('from', 'a'): 1, ('a', 'very'): 1, ('very', 'early'): 1, ('early', '</s>'): 1, ('<s>', 'Her'): 1, ('Her', 'mother'): 1, ('mother', 'had'): 1, ('had', 'died'): 1, ('died', 'too'): 1, ('too', 'long'): 1, ('long', 'ago'): 1, ('ago', 'for'): 1, ('for', 'her'): 1, ('to', 'have'): 1, ('have', 'more'): 1, ('more', 'than'): 1, ('than', 'an'): 1, ('an', 'indistinct'): 1, ('indistinct', 'remembrance'): 1, ('remembrance', 'of'): 1, ('her', 'caresses'): 1, ('caresses', 'and'): 1, ('and', 'her'): 1, ('her', 'place'): 1, ('place', 'had'): 1, ('been', 'supplied'): 1, ('supplied', 'by'): 1, ('by', 'an'): 1, ('an', 'excellent'): 1, ('excellent', 'woman'): 1, ('woman', 'as'): 1, ('as', 'governess'): 1, ('governess', 'who'): 1, ('who', 'had'): 1, ('had', 'fallen'): 1, ('fallen', 'little'): 1, ('little', 'short'): 1, ('short', 'of'): 1, ('a', 'mother'): 1, ('mother', 'in'): 1, ('<s>', 'Sixteen'): 1, ('Sixteen', 'years'): 1, ('years', 'had'): 1, ('had', 'Miss'): 1, ('Taylor', 'been'): 1, ('been', 'in'): 1, ('<s>', 'Woodhouse'): 1, ('Woodhouse', 's'): 1, ('s', 'family'): 1, ('family', 'less'): 1, ('less', 'as'): 1, ('as', 'a'): 1, ('a', 'governess'): 1, ('governess', 'than'): 1, ('than', 'a'): 1, ('a', 'friend'): 1, ('very', 'fond'): 1, ('fond', 'of'): 1, ('of', 'both'): 1, ('both', 'daughters'): 1, ('daughters', 'but'): 1, ('but', 'particularly'): 1, ('particularly', 'of'): 1, ('<s>', 'Between'): 1, ('Between', 'them'): 1, ('them', 'it'): 1, ('it', 'was'): 1, ('was', 'more'): 1, ('more', 'the'): 1, ('the', 'intimacy'): 1, ('intimacy', 'of'): 1, ('<s>', 'Even'): 1, ('Even', 'before'): 1, ('before', 'Miss'): 1, ('Taylor', 'had'): 1, ('had', 'ceased'): 1, ('ceased', 'to'): 1, ('to', 'hold'): 1, ('hold', 'the'): 1, ('the', 'nominal'): 1, ('nominal', 'office'): 1, ('office', 'of'): 1, ('of', 'governess'): 1, ('governess', 'the'): 1, ('the', 'mildness'): 1, ('mildness', 'of'): 1, ('her', 'temper'): 1, ('temper', 'had'): 1, ('had', 'hardly'): 1, ('hardly', 'allowed'): 1, ('allowed', 'her'): 1, ('to', 'impose'): 1, ('impose', 'any'): 1, ('any', 'restraint'): 1, ('restraint', 'and'): 1, ('and', 'the'): 1, ('the', 'shadow'): 1, ('shadow', 'of'): 1, ('of', 'authority'): 1, ('authority', 'being'): 1, ('being', 'now'): 1, ('now', 'long'): 1, ('long', 'passed'): 1, ('passed', 'away'): 1, ('away', 'they'): 1, ('they', 'had'): 1, ('been', 'living'): 1, ('living', 'together'): 1, ('together', 'as'): 1, ('as', 'friend'): 1, ('friend', 'and'): 1, ('and', 'friend'): 1, ('very', 'mutually'): 1, ('mutually', 'attached'): 1, ('attached', 'and'): 1, ('and', 'Emma'): 1, ('Emma', 'doing'): 1, ('doing', 'just'): 1, ('just', 'what'): 1, ('what', 'she'): 1, ('she', 'liked'): 1, ('liked', 'highly'): 1, ('highly', 'esteeming'): 1, ('esteeming', 'Miss'): 1, ('Taylor', 's'): 1, ('s', 'judgment'): 1, ('judgment', 'but'): 1, ('but', 'directed'): 1, ('directed', 'chiefly'): 1, ('chiefly', 'by'): 1, ('by', 'her'): 1, ('her', '</s>'): 1})\n",
            "Counter({('of',): 14, ('had',): 9, ('and',): 8, ('the',): 8, ('<s>',): 7, ('</s>',): 7, ('her',): 7, ('a',): 6, ('to',): 5, ('in',): 4, ('very',): 4, ('been',): 4, ('Emma',): 3, ('by',): 3, ('s',): 3, ('as',): 3, ('governess',): 3, ('Miss',): 3, ('Taylor',): 3, ('friend',): 3, ('I',): 2, ('Woodhouse',): 2, ('with',): 2, ('years',): 2, ('little',): 2, ('was',): 2, ('daughters',): 2, ('mother',): 2, ('long',): 2, ('more',): 2, ('than',): 2, ('an',): 2, ('but',): 2, ('Jane',): 1, ('Austen',): 1, ('1816',): 1, ('VOLUME',): 1, ('CHAPTER',): 1, ('handsome',): 1, ('clever',): 1, ('rich',): 1, ('comfortable',): 1, ('home',): 1, ('happy',): 1, ('disposition',): 1, ('seemed',): 1, ('unite',): 1, ('some',): 1, ('best',): 1, ('blessings',): 1, ('existence',): 1, ('lived',): 1, ('nearly',): 1, ('twenty',): 1, ('one',): 1, ('world',): 1, ('distress',): 1, ('or',): 1, ('vex',): 1, ('She',): 1, ('youngest',): 1, ('two',): 1, ('most',): 1, ('affectionate',): 1, ('indulgent',): 1, ('father',): 1, ('consequence',): 1, ('sister',): 1, ('marriage',): 1, ('mistress',): 1, ('his',): 1, ('house',): 1, ('from',): 1, ('early',): 1, ('Her',): 1, ('died',): 1, ('too',): 1, ('ago',): 1, ('for',): 1, ('have',): 1, ('indistinct',): 1, ('remembrance',): 1, ('caresses',): 1, ('place',): 1, ('supplied',): 1, ('excellent',): 1, ('woman',): 1, ('who',): 1, ('fallen',): 1, ('short',): 1, ('Sixteen',): 1, ('family',): 1, ('less',): 1, ('fond',): 1, ('both',): 1, ('particularly',): 1, ('Between',): 1, ('them',): 1, ('it',): 1, ('intimacy',): 1, ('Even',): 1, ('before',): 1, ('ceased',): 1, ('hold',): 1, ('nominal',): 1, ('office',): 1, ('mildness',): 1, ('temper',): 1, ('hardly',): 1, ('allowed',): 1, ('impose',): 1, ('any',): 1, ('restraint',): 1, ('shadow',): 1, ('authority',): 1, ('being',): 1, ('now',): 1, ('passed',): 1, ('away',): 1, ('they',): 1, ('living',): 1, ('together',): 1, ('mutually',): 1, ('attached',): 1, ('doing',): 1, ('just',): 1, ('what',): 1, ('she',): 1, ('liked',): 1, ('highly',): 1, ('esteeming',): 1, ('judgment',): 1, ('directed',): 1, ('chiefly',): 1})\n",
            "0.7400555739554517\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "ls = [[1,2], [1,2], [3,4], [5,6], [7,8], [0,0], [0,0], [0,0]]\n",
        "\n",
        "print(Counter([tuple(l) for l in ls]))"
      ],
      "metadata": {
        "id": "XkzAsCezlkf7",
        "outputId": "89519c3a-7536-462e-f020-a9510e85929c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({(0, 0): 3, (1, 2): 2, (3, 4): 1, (5, 6): 1, (7, 8): 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = [\"ha\"]*8\n",
        "print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMZzBdkzYQty",
        "outputId": "c771dccc-1d4b-4992-8d0e-227d4b367c38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ha', 'ha', 'ha', 'ha', 'ha', 'ha', 'ha', 'ha']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.9**: Run your $n$-gram program on two different small corpora of your choice (you might use email text or newsgroups). Now compare the statistics of the two\n",
        "corpora. What are the differences in the most common unigrams between the two? How about interesting differences in bigrams?"
      ],
      "metadata": {
        "id": "GQZIGvJfSlge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.10**: Add an option to your program to generate random sentences."
      ],
      "metadata": {
        "id": "OoKsGH6BSuyY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.11**: Add an option to your program to compute the perplexity of a test set."
      ],
      "metadata": {
        "id": "pk0F1Ti5S0Ew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.12**: You are given a training set of $100$ numbers that consists of $91$ zeros and $1$ each of the other digits $1-9$. Now we see the following test set: 0 0 0 0 0 3 0 0 0 0. What is the unigram perplexity?"
      ],
      "metadata": {
        "id": "QlgkusZqNayA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model trained using the skewed training set learns the following probabilities:\n",
        "\\begin{align}\n",
        "  P(0) &= .91 && &P(1) &= P(2) = \\dots = P(9) = .01\n",
        "\\end{align}\n",
        "\n",
        "Therefore, the unigram perplexity of the test set can be computed as follows:\n",
        "\\begin{align}\n",
        "  PP(W) &= P(0000030000)^{-\\frac{1}{10}} = \\left(P(0)^{9}P(3)\\right)^{-\\frac{1}{10}} \\approx 1.725\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "WqP4njuVNuJs"
      }
    }
  ]
}