{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peterchang0414/SLP/blob/main/Ch3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3: N-gram Language Models"
      ],
      "metadata": {
        "id": "qk47KCC8e3kD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solution by: Peter Chang (GItHub: @peterchang0414)\n",
        "\n",
        "*Last edited: 2021.12.21*"
      ],
      "metadata": {
        "id": "6A8G9Nch76Cq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1**: Write out the equation for trigram probability estimation (modifying Eq. 3.11). Now write out all the non-zero trigram probabilities for the *I am Sam* corpus on page 32."
      ],
      "metadata": {
        "id": "bxhs_F7yfDEK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a trigram model, we make the following approximation:\n",
        "\\begin{align}\n",
        "        P(w_n|w_{1:n-1}) \\approx P(w_n|w_{n-2}w_{n-1})\n",
        "\\end{align}\n",
        "As with the bigram model, to compute the trigram probability of a word $z$ given previous two words $x$ and $y$, we compute the count of the trigram $C(xyz)$ and normalize by ubigram count of $C(xy)$:\n",
        "\\begin{align}\n",
        "  P(w_n|w_{n-2}w_{n-1}) &= \\frac{C(w_{n-2}w_{n-1}w_{n})}{C(w_{n-2}w_{n-1})}\n",
        "\\end{align}\n",
        "\n",
        "For the *I am Sam* corpus, we augment each sentence with two specials at the beginning and at the end:\n",
        "\n",
        "\\begin{align}\n",
        "  &\\texttt{<s><s> I am Sam </s></s>} \\\\\n",
        "  &\\texttt{<s><s> Sam I am </s></s>} \\\\\n",
        "  &\\texttt{<s><s> I do not like green eggs and ham </s></s>}\n",
        "\\end{align}\n",
        "\n",
        "Then, the following are all the non-zero trigram probabilities for the corpus:\n",
        "\n",
        "\\begin{align}\n",
        "  P(\\texttt{I}|\\texttt{<s><s>}) &= \\frac{2}{3} = .67, && &P(\\texttt{Sam}|\\texttt{<s><s>}) &= \\frac{1}{3} = .33 && &P(\\texttt{am}|\\texttt{<s> I}) &= \\frac{1}{2}=.5 \\\\\n",
        "  P(\\texttt{I}|\\texttt{<s> Sam}) &= 1 && &P(\\texttt{do}|\\texttt{<s> I}) &= \\frac{1}{2} = .5 && &P(\\texttt{Sam}|\\texttt{I am}) &= \\frac{1}{2} = .5 \\\\\n",
        "  P(\\texttt{am}|\\texttt{Sam I}) &= 1 && &P(\\texttt{not}|\\texttt{I do}) &= 1 && &P(\\texttt{</s>}|\\texttt{am Sam}) &= 1 \\\\\n",
        "  P(\\texttt{</s>}|\\texttt{I am}) &= \\frac{1}{2} = .5 && &P(\\texttt{like}|\\texttt{do not}) &= 1 && &P(\\texttt{</s>}|\\texttt{Sam </s>}) &= 1 \\\\\n",
        "  P(\\texttt{</s>}|\\texttt{am </s>}) &= 1 && &P(\\texttt{green}|\\texttt{not like}) &= 1 && &P(\\texttt{eggs}|\\texttt{like green}) &= 1 \\\\\n",
        "  P(\\texttt{and}|\\texttt{green eggs}) &= 1 && &P(\\texttt{ham}|\\texttt{eggs and}) &= 1 && &P(\\texttt{</s>}|\\texttt{and ham}) &= 1 \\\\\n",
        "  P(\\texttt{</s>}|\\texttt{ham </s>}) &= 1\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "vkrdKv5_-vzM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.2**: Calculate the probability of the sentence $\\texttt{i want chinese food}$. Give two probabilities, one using Fig. 3.2 and the 'useful probabililties' just below it on page 34, and another using the add-1 smoothed table in Fig. 3.7. Assume the additional add-1 smoothed probabilities $P(\\texttt{i}|\\texttt{<s>}) = 0.19$ and $P(\\texttt{</s>}|\\texttt{food}) = 0.40$."
      ],
      "metadata": {
        "id": "Kg0BkgRuAMsz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Fig 3.2, we can compute the probability of the sentence as follows:\n",
        "\\begin{align}\n",
        "  P(\\texttt{<s> i want chinese food </s>}) &= P(\\texttt{i|<s>})\\cdot P(\\texttt{want|i}) \\cdot P(\\texttt{chinese|want}) \\cdot P(\\texttt{food|chinese}) \\cdot P(\\texttt{</s>|food}) \\\\\n",
        "  &= .25 \\times .33 \\times .0065 \\times .52 \\times .68 \\\\\n",
        "  &\\approx .00019\n",
        "\\end{align}\n",
        "\n",
        "Using the add-1 smoothed probabilities, we can compute the probability of the sentence as follows:\n",
        "\\begin{align}\n",
        "  P(\\texttt{<s> i want chinese food </s>}) &= P(\\texttt{i|<s>})\\cdot P(\\texttt{want|i}) \\cdot P(\\texttt{chinese|want}) \\cdot P(\\texttt{food|chinese}) \\cdot P(\\texttt{</s>|food}) \\\\\n",
        "  &= .19 \\times .21 \\times .0029 \\times .052 \\times .40 \\\\\n",
        "  &\\approx .0000024\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "s6tl1RiTCQg2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.3**: Which of the two probabilities your computed in the previous exercise is higher, unsmoothed or smoothed? Explain why."
      ],
      "metadata": {
        "id": "7ndt4KlSsmhy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The unsmoothed probability is higher than the smoothed probability. This is because all the bigram word pairs in the sentence had non-zero probability in the unsmoothed model, which means the smoothing operation took away probabilities from those pairs to add to the pairs which initially have probabilities of zero."
      ],
      "metadata": {
        "id": "upQfauyNsuED"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.4**: We are given the following corpus, modified from the one in the chapter:\n",
        "\\begin{align}\n",
        "  &\\texttt{<s> I am Sam </s>} \\\\\n",
        "  &\\texttt{<s> Sam I am </s>} \\\\\n",
        "  &\\texttt{<s> I am Sam </s>} \\\\\n",
        "  &\\texttt{<s> I do not like green eggs and Sam </s>} \n",
        "\\end{align}\n",
        "Using a bigram language model with add-one smoothing, what is $P(\\texttt{Sam|am})$? Include $\\texttt{<s>}$ and $\\texttt{</s>}$ in your counts just like any other token."
      ],
      "metadata": {
        "id": "oz_XxpYrtFvW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the size of the vocabulary, $V$ is given by:\n",
        "\\begin{align}\n",
        "  V &= |\\{\\texttt{I, am, Sam, do, not, like, green, eggs, and, <s>, </s>}\\}|= 11\n",
        "\\end{align}\n",
        "The add-one smoothed probability can be computed as follows:\n",
        "\\begin{align}\n",
        "  P_{Laplace}(\\texttt{Sam|am}) &= \\frac{C(\\texttt{am Sam})+1}{C(\\texttt{am}) + V} = \\frac{2+1}{3+11} = \\frac{3}{14} \\approx .214\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "vLt2YU39tr3v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.5**: Suppose we didn't use the end-symbol $\\texttt{</s>}$. Train an unsmoothed bigram grammar on the following training corpus without using the end-symbol $\\texttt{</s>}$:\n",
        "\\begin{align}\n",
        "  \\texttt{<s> a b} \\\\\n",
        "  \\texttt{<s> b b} \\\\\n",
        "  \\texttt{<s> b a} \\\\\n",
        "  \\texttt{<s> a a}\n",
        "\\end{align}\n",
        "Demonstrate that your bigram model does not assign a single probability distribution across all sentence lengths by showing that the sum of the probability\n",
        "of the four possible $2$ word sentences over the alphabet $\\texttt{{a,b}}$ is $1.0$, and the sum of the probability of all possible $3$ word sentences over the alphabet $\\texttt{{a,b}}$ is also $1.0$."
      ],
      "metadata": {
        "id": "PPrmUefoHcix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bigram probabilities for the training corpus are as follows:\n",
        "\\begin{align}\n",
        "  P(\\texttt{a|<s>}) &= \\frac{2}{4} = \\frac{1}{2} && &P(\\texttt{a|b}) &= \\frac{1}{2} && &P(\\texttt{a|a}) &= \\frac{1}{2} \\\\\n",
        "  P(\\texttt{b|<s>}) &= \\frac{2}{4} = \\frac{1}{2} && &P(\\texttt{b|a}) &= \\frac{1}{2} && &P(\\texttt{b|b}) &= \\frac{1}{2}\n",
        "\\end{align}\n",
        "Note that due to the lack of end-symbol $\\texttt{</s>}$, the letters at the end of each sentence are not counted in the unigram count for the denominator.\n",
        "\n",
        "Then, we can compute the following joint probabilities:\n",
        "\\begin{align}\n",
        "  P(\\texttt{<s> x y}) &\\approx P(\\texttt{x|<s>})\\cdot P(\\texttt{y|x}) = \\frac{1}{4} && (\\text{for }x, y \\in \\{a, b\\}^2) \n",
        "\\end{align}\n",
        "\n",
        "The sum of the probabilities of the all possible $2$-word sentences over $\\texttt{{a,b}}$ is thus computed as:\n",
        "\\begin{align}\n",
        "  \\sum_{x, y \\in \\{a, b\\}^2}P(\\texttt{<s> x y}) = 4 * \\frac{1}{4} = 1\n",
        "\\end{align}\n",
        "Similarly, we can compute the joint probabilities for $3$-word sentences:\n",
        "\\begin{align}\n",
        "  P(\\texttt{<s> x y z}) &\\approx P(\\texttt{x|<s>})P(\\texttt{y|x})P(\\texttt{z|y}) = \\frac{1}{8} && (\\text{for } x,y,z \\in \\{a, b\\}^{3})\n",
        "\\end{align}\n",
        "Therefore, we have:\n",
        "\\begin{align}\n",
        "  \\sum_{x, y, z \\in \\{a,b\\}^3}P(\\texttt{<s> x y z}) &= 8 * \\frac{1}{8} = 1\n",
        "\\end{align}\n",
        "Therefore, the bigram model does not assign a single proability distribution across all sentence lengths."
      ],
      "metadata": {
        "id": "4kNJ5N_tIJh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.6**: Suppose we train a trigram language model with add-one smoothing on a given corpus. The corpus contains $V$ word types. Express a formula for estimating $P(w_3|w_1, w_2)$ where $w_3$ is a word which follows the bigram $(w_1, w_2)$, in terms of various $n$-gram counts and $V$. Use the notation $c(w_1, w_2, w_3)$ to denote the number of times that trigram $(w_1, w_2, w_3)$ occurs in the corpus, and so on for bigrams and unigrams."
      ],
      "metadata": {
        "id": "QCAjVV-rP9ul"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.12**: You are given a training set of $100$ numbers that consists of $91$ zeros and $1$ each of the other digits $1-9$. Now we see the following test set: 0 0 0 0 0 3 0 0 0 0. What is the unigram perplexity?"
      ],
      "metadata": {
        "id": "QlgkusZqNayA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model trained using the skewed training set learns the following probabilities:\n",
        "\\begin{align}\n",
        "  P(0) &= .91 && &P(1) &= P(2) = \\dots = P(9) = .01\n",
        "\\end{align}\n",
        "\n",
        "Therefore, the unigram perplexity of the test set can be computed as follows:\n",
        "\\begin{align}\n",
        "  PP(W) &= P(0000030000)^{-\\frac{1}{10}} = \\left(P(0)^{9}P(3)\\right)^{-\\frac{1}{10}} \\approx 1.725\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "WqP4njuVNuJs"
      }
    }
  ]
}