{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peterchang0414/SpeechAndLanguageProcessing3ed/blob/main/Ch4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4: Naive Bayes and Sentiment Classification"
      ],
      "metadata": {
        "id": "qk47KCC8e3kD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solution by: Peter Chang (GItHub: @peterchang0414)\n",
        "\n",
        "*Last edited: 2022.1.21*"
      ],
      "metadata": {
        "id": "6A8G9Nch76Cq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.1**: Assume the following likelihoods for each word being part of a positive or negative movie review, and equal prior probabilities for each class.\n",
        "\\begin{array}{ c c c }\n",
        "  & \\text{pos} & \\text{neg} \\\\ \n",
        "  \\hline\n",
        "  \\text{I} & 0.09 & 0.16 \\\\  \n",
        "  \\text{always} & 0.07 & 0.06 \\\\\n",
        "  \\text{like} & 0.29 & 0.06 \\\\\n",
        "  \\text{foreign} & 0.04 & 0.15 \\\\\n",
        "  \\text{films} & 0.08 & 0.11   \n",
        "\\end{array}\n",
        "What class will Naive bayes assign to the sentence \"I always like foreign films.\"?"
      ],
      "metadata": {
        "id": "bxhs_F7yfDEK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using equation (4.10), since we assume equal prior probabilities for each class:\n",
        "\\begin{align}\n",
        "  c_{NB} = \\text{argmax}_{c \\in C}\\sum_{i \\in \\text{positions}} \\log{P(w_i|c)}\n",
        "\\end{align}\n",
        "Computing the log probabilities for the positive class:\n",
        "\\begin{align}\n",
        "  \\sum_{i \\in \\text{positions}} \\log{P(w_i|\\text{pos})} &= \\log(0.09) + \\dots + \\log(0.08) \\approx -5.233\n",
        "\\end{align}\n",
        "whereas for the negative class we have:\n",
        "\\begin{align}\n",
        "  \\sum_{i \\in \\text{positions}} \\log{P(w_i|\\text{neg})} &= \\log(0.16) + \\dots + \\log(0.11) \\approx -5.022\n",
        "\\end{align}\n",
        "Therefore, Naive bayes will assign the sentence to the negative class."
      ],
      "metadata": {
        "id": "c7IiC777E-rv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.2**: Given the following short movie reviews, each labeled with a genre, either comedy or action:\n",
        "\n",
        "\n",
        "1.   fun, couple, love, love: **comedy**\n",
        "2.   fast, furious, shoot: **action**\n",
        "3.   couple, fly, fast, fun, fun: **comedy**\n",
        "4.   furious, shoot, shoot, fun: **action**\n",
        "5.   fly, fast, shoot, love: **action**\n",
        "\n",
        "and a new document D:\n",
        "\n",
        "> fast, couple, shoot, fly\n",
        "\n",
        "compute the most likely class for D. Assume a naive Bayes classifier and use add-1 smoothing for the likelihoods."
      ],
      "metadata": {
        "id": "-TaqbgDr4G5e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The prior for the two classes is computed:\n",
        "$$\n",
        "\\begin{align}\n",
        "  P(\\text{comedy}) &= \\frac{2}{5}, & P(\\text{action}) &= \\frac{3}{5}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "The probabilities for the words in the document D are as follows:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "  P(\\text{fast}|\\text{comedy}) &= \\frac{1+1}{9+7}=\\frac{2}{16}, & P(\\text{fast}|\\text{action}) &= \\frac{2+1}{11+7}=\\frac{3}{18} \\\\\n",
        "  P(\\text{couple}|\\text{comedy}) &= \\frac{2+1}{9+7}=\\frac{3}{16}, & P(\\text{couple}|\\text{action}) &= \\frac{0+1}{11+7}=\\frac{1}{18} \\\\\n",
        "  P(\\text{shoot}|\\text{comedy}) &= \\frac{0+1}{9+7}=\\frac{1}{16}, & P(\\text{shoot}|\\text{action}) &= \\frac{4+1}{11+7}=\\frac{5}{18} \\\\\n",
        "  P(\\text{fly}|\\text{comedy}) &= \\frac{1+1}{9+7}=\\frac{2}{16}, & P(\\text{fly}|\\text{action}) &= \\frac{1+1}{11+7}=\\frac{2}{18}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Therefore, the posterior probabilities are computed:\n",
        "$$\n",
        "\\begin{align}\n",
        "  P(\\text{comedy})P(D|\\text{comedy}) &= \\frac{2}{5} \\times \\frac{12}{16^{4}} \\approx 7.3 \\times 10^{-5} \\\\\n",
        "  P(\\text{action})P(D|\\text{action}) &= \\frac{3}{5} \\times \\frac{30}{18^{4}} \\approx 1.7 \\times 10^{-4}\n",
        "\\end{align}\n",
        "$$\n",
        "The model thus predicts the class *action* for the document $D$."
      ],
      "metadata": {
        "id": "-pjM-hQVkoeU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.3**: Train two models, multinomial naive Bayes and binarized naive Bayes, both with add-1 smoothing, on the following document counts for key sentiment words, with positive or negative class assigned as noted.\n",
        "\n",
        "$$\n",
        "\\begin{array}{ l l l l l}\n",
        " \\text{doc} & ``\\text{good\"} & ``\\text{poor\"} & ``\\text{great\"}&(\\text{class})\\\\ \n",
        " \\text{d1.} & 3 & 0 & 3 & \\text{pos}\\\\  \n",
        " \\text{d2.} & 0 & 1 & 2 & \\text{pos}\\\\\n",
        " \\text{d3.} & 1 & 3 & 0 & \\text{neg}\\\\\n",
        " \\text{d4.} & 1 & 5 & 2 & \\text{neg}\\\\\n",
        " \\text{d5.} & 0 & 2 & 0 & \\text{neg}\n",
        "\\end{array}\n",
        "$$\n",
        "Use both naive Bayes models to assign a class (pos or neg) to this sentence:\n",
        "\n",
        "> A good, good plot and great characters, but poor acting.\n",
        "\n",
        "Recall from page 62 that with naive Bayes text classification, we simply ignore (throw out) any word that never occurred in the training document. (We don't throw out words that appear in some classes but not others; that's what add-one smoothing is for.) Do the two models agree or disagree?\n"
      ],
      "metadata": {
        "id": "sxJSBAl2xxGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "def train_naive_bayes(docs, labels, binarize = False):\n",
        "  \"\"\"Train Naive Bayes model on given docs with labels\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  docs : list\n",
        "    list of documents, each of which is a dictionary of\n",
        "    unique word counts\n",
        "  labels : list\n",
        "    labels for the corresponding (index-matched) document\n",
        "    in the docs list\n",
        "  binarize : bool\n",
        "    indicates whether we use binarized Naive Bayes model\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  log_prior\n",
        "    dict where the keys are each class and values are\n",
        "    the log prior probability of each class\n",
        "  log_likelihood\n",
        "    dict of dict where the keys are each class and values are\n",
        "    dicts of log likelihood probability for each word in vocabulary\n",
        "  vocab\n",
        "    set of unique words in the union of docs\n",
        "  classes\n",
        "    set of unique labels\n",
        "  \"\"\"\n",
        "  if binarize:\n",
        "    for doc in docs:\n",
        "      for k, v in doc.items():\n",
        "        if v > 1:\n",
        "          doc[k] = 1\n",
        "  log_prior, log_likelihood, vocab = {}, {}, set()\n",
        "  for doc in docs:\n",
        "    vocab = vocab.union(set(list(doc.keys())))\n",
        "  classes = set(labels)\n",
        "  for c in classes:\n",
        "    log_likelihood[c] = {}\n",
        "    n_c = sum(map(lambda x : x == c, labels))\n",
        "    log_prior[c] = np.log(n_c / len(docs))\n",
        "    c_index = [i for i in range(len(label)) if label[i] == c]\n",
        "    big_doc = Counter()\n",
        "    for i in c_index:\n",
        "      big_doc.update(Counter(docs[i]))\n",
        "    for word in vocab:\n",
        "      log_likelihood[c][word] = np.log((big_doc[word]+1)/(sum([big_doc[w] + 1 for w in vocab])))\n",
        "  return log_prior, log_likelihood, vocab, classes\n",
        "\n",
        "def test_naive_bayes(docs, labels, doc, binarize=False):\n",
        "  \"\"\"Test Naive Bayes model on given doc\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  docs : list\n",
        "    list of documents, each of which is a dictionary of\n",
        "    unique word counts\n",
        "  labels : list\n",
        "    labels for the corresponding (index-matched) document\n",
        "    in the docs list\n",
        "  doc : str\n",
        "    test document/sentence\n",
        "  binarize : bool\n",
        "    indicates whether we use binarized Naive Bayes model\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  str\n",
        "    class label to which the Naive Bayes model classified\n",
        "    the test document\n",
        "  \"\"\"\n",
        "  lp, ll, vocab, classes = train_naive_bayes(docs, labels, binarize)\n",
        "  def process_doc(d, bin):\n",
        "    dd = ''.join(ch for ch in d.lower() if ch.isalnum() or ch == ' ')\n",
        "    ct = dict(Counter(dd.split()))\n",
        "    if bin:\n",
        "      for k, v in ct.items():\n",
        "        if v > 1:\n",
        "          ct[k] = 1\n",
        "    return ct\n",
        "  test_doc = process_doc(doc, binarize)\n",
        "  sum_class = {}\n",
        "  for c in classes:\n",
        "    sum_class[c] = lp[c]\n",
        "    for k, v in test_doc.items():\n",
        "      if k in vocab:\n",
        "        sum_class[c] += ll[c][k]*v\n",
        "  return max(sum_class, key=sum_class.get)"
      ],
      "metadata": {
        "id": "nwHZdGN2B6rb"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = [{'good': 3, 'poor': 0, 'great': 3},\n",
        "         {'good': 0, 'poor': 1, 'great': 2},\n",
        "         {'good': 1, 'poor': 3, 'great': 0},\n",
        "         {'good': 1, 'poor': 5, 'great': 2},\n",
        "         {'good': 0, 'poor': 2, 'great': 0}]\n",
        "labels = ['pos', 'pos', 'neg', 'neg', 'neg']\n",
        "test = 'A good, good plot and great characters, but poor acting.'\n",
        "\n",
        "print(f'Naive Bayes classification of test sentence:\\n\\t{test}')\n",
        "print('-'*65)\n",
        "print(f'Non-binarized: {test_naive_bayes(train, labels, test)}')\n",
        "print(f'Binarized: {test_naive_bayes(train, labels, test, True)}')"
      ],
      "metadata": {
        "id": "zWvbQOISPPD9",
        "outputId": "723a804a-cc0b-4038-96ec-71b267c9b3f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes classification of test sentence:\n",
            "\tA good, good plot and great characters, but poor acting.\n",
            "-----------------------------------------------------------------\n",
            "Non-binarized: pos\n",
            "Binarized: neg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the two models disagree: the non-binarzied model classifies the test sentence as positive while the binarized model classifies it as negative.\n",
        "\n",
        "Inspecting the test sentence, this makes intuitive sense, since the test sentence repeats the word \"good\" twice."
      ],
      "metadata": {
        "id": "HbCvFgAzO4eC"
      }
    }
  ]
}