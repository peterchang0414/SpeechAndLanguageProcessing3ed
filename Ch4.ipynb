{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peterchang0414/SpeechAndLanguageProcessing3ed/blob/main/Ch4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4: Naive Bayes and Sentiment Classification"
      ],
      "metadata": {
        "id": "qk47KCC8e3kD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solution by: Peter Chang (GItHub: @peterchang0414)\n",
        "\n",
        "*Last edited: 2022.1.3*"
      ],
      "metadata": {
        "id": "6A8G9Nch76Cq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.1**: Assume the following likelihoods for each word being part of a positive or negative movie review, and equal prior probabilities for each class.\n",
        "\\begin{array}{ c c c }\n",
        "  & \\text{pos} & \\text{neg} \\\\ \n",
        "  \\hline\n",
        "  \\text{I} & 0.09 & 0.16 \\\\  \n",
        "  \\text{always} & 0.07 & 0.06 \\\\\n",
        "  \\text{like} & 0.29 & 0.06 \\\\\n",
        "  \\text{foreign} & 0.04 & 0.15 \\\\\n",
        "  \\text{films} & 0.08 & 0.11   \n",
        "\\end{array}\n",
        "What class will Naive bayes assign to the sentence \"I always like foreign films.\"?"
      ],
      "metadata": {
        "id": "bxhs_F7yfDEK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.2**: Calculate the probability of the sentence $\\texttt{i want chinese food}$. Give two probabilities, one using Fig. 3.2 and the 'useful probabililties' just below it on page 34, and another using the add-1 smoothed table in Fig. 3.7. Assume the additional add-1 smoothed probabilities $P(\\texttt{i}|\\texttt{<s>}) = 0.19$ and $P(\\texttt{</s>}|\\texttt{food}) = 0.40$."
      ],
      "metadata": {
        "id": "Kg0BkgRuAMsz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Fig 3.2, we can compute the probability of the sentence as follows:\n",
        "\\begin{align}\n",
        "  P(\\texttt{<s> i want chinese food </s>}) &= P(\\texttt{i|<s>})\\cdot P(\\texttt{want|i}) \\cdot P(\\texttt{chinese|want}) \\cdot P(\\texttt{food|chinese}) \\cdot P(\\texttt{</s>|food}) \\\\\n",
        "  &= .25 \\times .33 \\times .0065 \\times .52 \\times .68 \\\\\n",
        "  &\\approx .00019\n",
        "\\end{align}\n",
        "\n",
        "Using the add-1 smoothed probabilities, we can compute the probability of the sentence as follows:\n",
        "\\begin{align}\n",
        "  P(\\texttt{<s> i want chinese food </s>}) &= P(\\texttt{i|<s>})\\cdot P(\\texttt{want|i}) \\cdot P(\\texttt{chinese|want}) \\cdot P(\\texttt{food|chinese}) \\cdot P(\\texttt{</s>|food}) \\\\\n",
        "  &= .19 \\times .21 \\times .0029 \\times .052 \\times .40 \\\\\n",
        "  &\\approx .0000024\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "s6tl1RiTCQg2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.3**: Which of the two probabilities your computed in the previous exercise is higher, unsmoothed or smoothed? Explain why."
      ],
      "metadata": {
        "id": "7ndt4KlSsmhy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The unsmoothed probability is higher than the smoothed probability. This is because all the bigram word pairs in the sentence had non-zero probability in the unsmoothed model, which means the smoothing operation took away probabilities from those pairs to add to the pairs which initially have probabilities of zero."
      ],
      "metadata": {
        "id": "upQfauyNsuED"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.4**: We are given the following corpus, modified from the one in the chapter:\n",
        "\\begin{align}\n",
        "  &\\texttt{<s> I am Sam </s>} \\\\\n",
        "  &\\texttt{<s> Sam I am </s>} \\\\\n",
        "  &\\texttt{<s> I am Sam </s>} \\\\\n",
        "  &\\texttt{<s> I do not like green eggs and Sam </s>} \n",
        "\\end{align}\n",
        "Using a bigram language model with add-one smoothing, what is $P(\\texttt{Sam|am})$? Include $\\texttt{<s>}$ and $\\texttt{</s>}$ in your counts just like any other token."
      ],
      "metadata": {
        "id": "oz_XxpYrtFvW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the size of the vocabulary, $V$ is given by:\n",
        "\\begin{align}\n",
        "  V &= |\\{\\texttt{I, am, Sam, do, not, like, green, eggs, and, <s>, </s>}\\}|= 11\n",
        "\\end{align}\n",
        "The add-one smoothed probability can be computed as follows:\n",
        "\\begin{align}\n",
        "  P_{Laplace}(\\texttt{Sam|am}) &= \\frac{C(\\texttt{am Sam})+1}{C(\\texttt{am}) + V} = \\frac{2+1}{3+11} = \\frac{3}{14} \\approx .214\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "vLt2YU39tr3v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.5**: Suppose we didn't use the end-symbol $\\texttt{</s>}$. Train an unsmoothed bigram grammar on the following training corpus without using the end-symbol $\\texttt{</s>}$:\n",
        "\\begin{align}\n",
        "  \\texttt{<s> a b} \\\\\n",
        "  \\texttt{<s> b b} \\\\\n",
        "  \\texttt{<s> b a} \\\\\n",
        "  \\texttt{<s> a a}\n",
        "\\end{align}\n",
        "Demonstrate that your bigram model does not assign a single probability distribution across all sentence lengths by showing that the sum of the probability\n",
        "of the four possible $2$ word sentences over the alphabet $\\texttt{{a,b}}$ is $1.0$, and the sum of the probability of all possible $3$ word sentences over the alphabet $\\texttt{{a,b}}$ is also $1.0$."
      ],
      "metadata": {
        "id": "PPrmUefoHcix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bigram probabilities for the training corpus are as follows:\n",
        "\\begin{align}\n",
        "  P(\\texttt{a|<s>}) &= \\frac{2}{4} = \\frac{1}{2} && &P(\\texttt{a|b}) &= \\frac{1}{2} && &P(\\texttt{a|a}) &= \\frac{1}{2} \\\\\n",
        "  P(\\texttt{b|<s>}) &= \\frac{2}{4} = \\frac{1}{2} && &P(\\texttt{b|a}) &= \\frac{1}{2} && &P(\\texttt{b|b}) &= \\frac{1}{2}\n",
        "\\end{align}\n",
        "Note that due to the lack of end-symbol $\\texttt{</s>}$, the letters at the end of each sentence are not counted in the unigram count for the denominator.\n",
        "\n",
        "Then, we can compute the following joint probabilities:\n",
        "\\begin{align}\n",
        "  P(\\texttt{<s> x y}) &\\approx P(\\texttt{x|<s>})\\cdot P(\\texttt{y|x}) = \\frac{1}{4} && (\\text{for }x, y \\in \\{a, b\\}^2) \n",
        "\\end{align}\n",
        "\n",
        "The sum of the probabilities of the all possible $2$-word sentences over $\\texttt{{a,b}}$ is thus computed as:\n",
        "\\begin{align}\n",
        "  \\sum_{x, y \\in \\{a, b\\}^2}P(\\texttt{<s> x y}) = 4 * \\frac{1}{4} = 1\n",
        "\\end{align}\n",
        "Similarly, we can compute the joint probabilities for $3$-word sentences:\n",
        "\\begin{align}\n",
        "  P(\\texttt{<s> x y z}) &\\approx P(\\texttt{x|<s>})P(\\texttt{y|x})P(\\texttt{z|y}) = \\frac{1}{8} && (\\text{for } x,y,z \\in \\{a, b\\}^{3})\n",
        "\\end{align}\n",
        "Therefore, we have:\n",
        "\\begin{align}\n",
        "  \\sum_{x, y, z \\in \\{a,b\\}^3}P(\\texttt{<s> x y z}) &= 8 * \\frac{1}{8} = 1\n",
        "\\end{align}\n",
        "Therefore, the bigram model does not assign a single proability distribution across all sentence lengths."
      ],
      "metadata": {
        "id": "4kNJ5N_tIJh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.6**: Suppose we train a trigram language model with add-one smoothing on a given corpus. The corpus contains $V$ word types. Express a formula for estimating $P(w_3|w_1 w_2)$ where $w_3$ is a word which follows the bigram $(w_1, w_2)$, in terms of various $n$-gram counts and $V$. Use the notation $c(w_1, w_2, w_3)$ to denote the number of times that trigram $(w_1, w_2, w_3)$ occurs in the corpus, and so on for bigrams and unigrams."
      ],
      "metadata": {
        "id": "QCAjVV-rP9ul"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extending the argument for the bigram model, we have:\n",
        "\\begin{align}\n",
        "  P_{Laplace}(w_3|w_1w_2) &= \\frac{c(w_1,w_2,w_3)+1}{\\sum_{w}\\left(c(w_1,w_2,w)+1\\right)} = \\frac{c(w_1,w_2,w_3)+1}{c(w_1,w_2)+V}\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "aSziYVE9QlZQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.7**: We are given the following corpus, modified from the one in the chapter:\n",
        "\\begin{align}\n",
        "  &\\texttt{<s> I am Sam </s>} \\\\\n",
        "  &\\texttt{<s> Sam I am </s>} \\\\\n",
        "  &\\texttt{<s> I am Sam </s>} \\\\\n",
        "  &\\texttt{<s> I do not like green eggs and Sam </s>} \n",
        "\\end{align}\n",
        "If we use linear interpolation smoothing between a maximum-likelihood bigram model and a maximum-likelihood unigram model with $\\lambda_1 = \\frac{1}{2}$ and $\\lambda_2 = \\frac{1}{2}$, what is $P(\\texttt{Sam|am})$? Include $\\texttt{<s>}$ and $\\texttt{</s>}$ in your counts just like any other token."
      ],
      "metadata": {
        "id": "OjqgJ3E5RxB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Counting from the corpus, we have the following probabilities:\n",
        "\\begin{align}\n",
        "  P(\\texttt{Sam|am}) &= \\frac{2}{3} \\\\\n",
        "  P(\\texttt{Sam}) &= \\frac{4}{25}\n",
        "\\end{align}\n",
        "Using these values, we can compute the linear interpolation smoothing probability:\n",
        "\\begin{align}\n",
        "  \\hat{P}(\\texttt{Sam|am}) &= \\lambda_1 P(\\texttt{Sam|am}) + \\lambda_2 P(\\texttt{Sam}) = \\frac{1}{3} + \\frac{2}{25} \\approx .41\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "JtIHD7O8SeRW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.8**: Write a program to compute unsmoothed unigrams and bigrams."
      ],
      "metadata": {
        "id": "62WSb_E8Sgz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "def process_corpus(corpus, n=2, incl_period=False):\n",
        "  \"\"\"Process corpus by lowercasing everything and inserting special symbols\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  corpus : str\n",
        "    corpus text where sentences are\n",
        "    separated by periods.\n",
        "  incl_period : bool\n",
        "    whether to include period in unigram counts\n",
        "    only applies to n == 1 case\n",
        "  n : int\n",
        "    parameter n of n-gram approximation\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  list\n",
        "    list of lower-cased sentences with special symbols inserted\n",
        "  \"\"\"\n",
        "  if n == 1:\n",
        "    result = []\n",
        "    curr_str = \"\"\n",
        "    for c in corpus:\n",
        "      if c.isalnum():\n",
        "        curr_str += c\n",
        "      else:\n",
        "        if curr_str:\n",
        "          result.append(curr_str.lower())\n",
        "        if incl_period and c == \".\":\n",
        "          result.append(c)\n",
        "        curr_str = \"\"\n",
        "    if curr_str:\n",
        "      result.append(curr_str.lower())\n",
        "    return result\n",
        "  \n",
        "  result = []\n",
        "  tmp_ls = [\"<s>\"]*(n-1)\n",
        "  tmp_str = \"\"\n",
        "  for c in corpus:\n",
        "    if c.isalnum():\n",
        "      tmp_str += c\n",
        "    else:\n",
        "      if tmp_str:\n",
        "        tmp_ls.append(tmp_str.lower())\n",
        "      if c == '.':\n",
        "        result.append(tmp_ls + [\"</s>\"]*(n-1))\n",
        "        tmp_ls = [\"<s>\"]*(n-1)\n",
        "      tmp_str = \"\"\n",
        "\n",
        "  # If final sentence does not end with a period, add to list\n",
        "  if tmp_str:\n",
        "    tmp_ls.append(tmp_str.lower())\n",
        "  if tmp_ls != [\"<s>\"]*(n-1):\n",
        "    result.append(tmp_ls + [\"</s>\"]*(n-1))\n",
        "  return result\n",
        "\n",
        "def ngram(processed_corpus, words, n=2):\n",
        "  \"\"\"Compute ngram of curr_word given prev_words trained on corpus.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  processed_corpus : list\n",
        "    processed corpus text\n",
        "  words : list\n",
        "    list of words to compute ngram probability\n",
        "  n : int\n",
        "    parameter n of n-gram approximation\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  double\n",
        "    the ngram probability P(words)\n",
        "  \"\"\"\n",
        "  # lowercase the words:\n",
        "  words = [word.lower() for word in words]\n",
        "  # Raise error if length of words < n\n",
        "  if len(words) < n:\n",
        "    print(\"Please provide list of words of adequate size.\")\n",
        "    return\n",
        "  #ngram counts\n",
        "  counter_n = Counter()\n",
        "  for sentence in processed_corpus:\n",
        "    for i in range(len(sentence)-n+1):\n",
        "      counter_n[tuple(sentence[i:i+n])] += 1\n",
        "  #(n-1)gram counts (only if n > 1)\n",
        "  counter_n1 = Counter()\n",
        "  if n > 1:\n",
        "    for sentence in processed_corpus:\n",
        "      for i in range(len(sentence)-n+2):\n",
        "        counter_n1[tuple(sentence[i:i+n-1])] += 1\n",
        "\n",
        "  # Compute log of ngram conditional probability\n",
        "  def ngram_log_prob(counter_n, counter_n1, curr_word, prev_words):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    counter_n : Counter\n",
        "      ngram Counter of corpus\n",
        "    counter_n1 : Counter\n",
        "      (n-1)gram Counter of corpus\n",
        "      None if n == 1\n",
        "    prev_words : list\n",
        "      previous words to condition on\n",
        "      empty if n == 1\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    double\n",
        "      the log10 of ngram conditional probability P(curr_word|prev_words)\n",
        "    \"\"\"\n",
        "    ngram_phrase = tuple([w for w in prev_words] + [curr_word])\n",
        "    # if phrase is not in corpus, return neg infinity\n",
        "    if ngram_phrase not in counter_n:\n",
        "      return float('-inf')\n",
        "    # if unigram, return simple probability value\n",
        "    if not prev_words:\n",
        "      return math.log10((counter_n[tuple(curr_word)])/(sum(counter_n.values())))\n",
        "    return math.log10((counter_n[ngram_phrase])/(counter_n1[tuple(prev_words)]))\n",
        "  \n",
        "  log_result = 0\n",
        "  for i in range(len(words)-n+1):\n",
        "    log_result += ngram_log_prob(counter_n, counter_n1, words[i+n-1], words[i:i+n-1])\n",
        "  return 10**log_result"
      ],
      "metadata": {
        "id": "kD2I8vzfVJE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check\n",
        "corpus = \"I am Sam. Sam I am. I do not like green eggs and ham.\"\n",
        "assert ngram(process_corpus(corpus, 2), ['I', 'am'], 2) == 2/3\n",
        "assert ngram(process_corpus(corpus, 2), ['am', 'Sam'], 2) == 1/2\n",
        "print(\"Assertion passed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gq_zxnHYrNYE",
        "outputId": "42900319-5e2b-4c5b-cb32-c26f2f56642b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assertion passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.9**: Run your $n$-gram program on two different small corpora of your choice (you might use email text or newsgroups). Now compare the statistics of the two\n",
        "corpora. What are the differences in the most common unigrams between the two? How about interesting differences in bigrams?"
      ],
      "metadata": {
        "id": "GQZIGvJfSlge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def most_common_ngram(corpus, n=1, k=1):\n",
        "  \"\"\"Compute k most common ngrams of corpus.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  corpus : str\n",
        "    corpus text\n",
        "  n : int\n",
        "    parameter n of ngram\n",
        "  k : int\n",
        "    number of most common unigrams to return\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  \n",
        "  list\n",
        "    list of tuples(ngram, ngram count) for\n",
        "    k most common ngrams\n",
        "  \"\"\"\n",
        "  proc_corpus = process_corpus(corpus, n)\n",
        "  if n == 1:\n",
        "    c = Counter(proc_corpus)\n",
        "  else:\n",
        "    c = Counter()\n",
        "    for sentence in proc_corpus:\n",
        "      for i in range(len(sentence)-n+1):\n",
        "        c[tuple(sentence[i:i+n])] += 1\n",
        "  return c.most_common()[:k]"
      ],
      "metadata": {
        "id": "PBe40nI9z_iG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "from nltk.corpus import gutenberg\n",
        "import pprint\n",
        "\n",
        "# Run ngram on Emma (Jane Austen) and Macbeth (Shakespeare)\n",
        "emma_raw, macbeth_raw = gutenberg.raw('austen-emma.txt'), gutenberg.raw('shakespeare-macbeth.txt')\n",
        "\n",
        "print(most_common_ngram(emma_raw, 2, 5))\n",
        "print(most_common_ngram(macbeth_raw, 2, 5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ux0KCDDSvt2K",
        "outputId": "f5de7188-e77d-4e23-8135-ffba5d67febe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "['emma', 'by', 'jane', 'austen', '1816', 'volume', 'i', 'chapter', 'i', 'emma', 'woodhouse', 'handsome', 'clever', 'and', 'rich', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', 'and', 'had', 'lived', 'nearly', 'twenty', 'one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', '.', 'she', 'was', 'the', 'youngest', 'of', 'the', 'two', 'daughters', 'of', 'a', 'most', 'affectionate', 'indulgent', 'father', 'and', 'had', 'in', 'consequence', 'of', 'her', 'sister', 's', 'marriage', 'been', 'mistress', 'of', 'his', 'house', 'from', 'a', 'very', 'early', 'period', '.', 'her', 'mother', 'had', 'died', 'too', 'long', 'ago', 'for', 'her', 'to', 'have', 'more', 'than', 'an', 'indistinct', 'remembrance', 'of', 'her', 'caresses', 'and', 'her', 'place', 'had', 'been', 'supplied', 'by', 'an', 'excellent', 'woman', 'as', 'governess', 'who', 'had', 'fallen', 'little', 'short', 'of', 'a', 'mother', 'in', 'affection', '.', 'sixteen', 'years', 'had', 'miss', 'taylor', 'been', 'in', 'mr', '.', 'woodhouse', 's', 'family', 'less', 'as', 'a', 'governess', 'than', 'a', 'friend', 'very', 'fond', 'of', 'both', 'daughters', 'but', 'particularly', 'of', 'emma', '.', 'between', 'them', 'it', 'was', 'more', 'the', 'intimacy', 'of', 'sisters', '.', 'even', 'before', 'miss', 'taylor', 'had', 'ceased', 'to', 'hold', 'the', 'nominal', 'office', 'of', 'governess', 'the', 'mildness', 'o']\n",
            "[(('mr', '</s>'), 1154), (('<s>', 'i'), 853), (('mrs', '</s>'), 701), (('to', 'be'), 608), (('of', 'the'), 561)]\n",
            "[(('macb', '</s>'), 137), (('<s>', 'enter'), 73), (('<s>', 'i'), 70), (('macd', '</s>'), 58), (('<s>', 'what'), 50)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.10**: Add an option to your program to generate random sentences."
      ],
      "metadata": {
        "id": "OoKsGH6BSuyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "def unigram_generate_sentences(corpus, num_sent):\n",
        "  \"\"\"Generate random sentences trained on corpus\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  corpus : str\n",
        "    corpus text\n",
        "  num_sent: int\n",
        "    number of sentences to generate\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  str\n",
        "    sentences generated according to unigram probability distribution\n",
        "  \"\"\"\n",
        "  proc_corpus = process_corpus(corpus, 1, True)\n",
        "  curr_sent, capital, res = 0, True, \"\"\n",
        "  c = Counter(proc_corpus)\n",
        "  total = sum(c.values())\n",
        "  prob_dist = [c[word]/total for word in c]\n",
        "  while curr_sent < num_sent:\n",
        "    curr = np.random.choice(list(c.keys()), p=prob_dist)\n",
        "    if curr == \".\":\n",
        "      if capital:\n",
        "        continue\n",
        "      else:\n",
        "        curr_sent += 1\n",
        "        capital = True\n",
        "        res = res[:-1] + \". \"\n",
        "    else:\n",
        "      if capital:\n",
        "        curr = curr[0].upper() + curr[1:]\n",
        "      res += curr + \" \"\n",
        "      capital = False\n",
        "  return res\n",
        "\n",
        "def ngram_generate_sentences(corpus, num_sent, n=2):\n",
        "  \"\"\"Generate random sentences trained on corpus\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  corpus : str\n",
        "    corpus text\n",
        "  num_sent: int\n",
        "    number of sentences to generate\n",
        "  n : int\n",
        "    parameter n of ngram\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  str\n",
        "    sentences generated according to ngram probability distribution\n",
        "  \"\"\"\n",
        "  if n == 1:\n",
        "    return unigram_generate_sentences(corpus, num_sent)\n",
        "\n",
        "  proc_corpus = process_corpus(corpus, n)\n",
        "  res, ngram_prob = [], {}\n",
        "  # ngram, (n-1)gram counts\n",
        "  n1_counter, n_counter = Counter(), Counter()\n",
        "  for sentence in proc_corpus:\n",
        "    for i in range(len(sentence)-n+2):\n",
        "      if i < len(sentence)-n+1:\n",
        "        n_counter[tuple(sentence[i:i+n])] += 1\n",
        "      n1_counter[tuple(sentence[i:i+n-1])] += 1\n",
        "  # Compute ngram probabilities for all existing n-tuples in corpus\n",
        "  for sentence in proc_corpus:\n",
        "    for i in range(len(sentence)-n+1):\n",
        "      ngram_prob[tuple(sentence[i:i+n])] = n_counter[tuple(sentence[i:i+n])]/n1_counter[tuple(sentence[i:i+n-1])]\n",
        "  # Generate sentences\n",
        "  curr_sent = 0\n",
        "  prev, res, capital = (\"<s>\",)*(n-1), \"\", True\n",
        "  while curr_sent < num_sent:\n",
        "    choices = [p for p in list(ngram_prob.keys()) if p[:n-1] == prev]\n",
        "    ind = [i for i in range(len(choices))]\n",
        "    dist = [ngram_prob[choice] for choice in choices]\n",
        "    curr = choices[np.random.choice(ind, p=dist)]\n",
        "    if curr[-1] == \"</s>\":\n",
        "      res += \".\"\n",
        "      capital = True\n",
        "      prev = (\"<s>\",)*(n-1)\n",
        "      curr_sent += 1\n",
        "    else:\n",
        "      curr_text = \"\"\n",
        "      if capital:\n",
        "        curr_text = \" \".join(curr[n-1:])\n",
        "        curr_text = curr_text[0].upper() + curr_text[1:]\n",
        "      else:\n",
        "        curr_text = curr[-1]\n",
        "      res += \" \" +curr_text\n",
        "      capital = False\n",
        "      prev = curr[1:]\n",
        "  return res\n"
      ],
      "metadata": {
        "id": "JOGJhiRv6Byl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "from nltk.corpus import gutenberg\n",
        "import pprint\n",
        "\n",
        "# Run ngram on Emma (Jane Austen) and Macbeth (Shakespeare)\n",
        "emma_raw, macbeth_raw = gutenberg.raw('austen-emma.txt'), gutenberg.raw('shakespeare-macbeth.txt')\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "\n",
        "pp.pprint(ngram_generate_sentences(emma_raw, 10, n=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ne63kzUiJuQl",
        "outputId": "1e310eea-b22a-4611-8b66-12a31aef0250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "(' The decree is issued by somebody. Churchill lived i suppose there is not '\n",
            " 'such another nursery establishment so liberal and so fond of each other. '\n",
            " 'Weston was afraid of draughts for the young people will find a way. Weston s '\n",
            " 'arguments against herself. Knightley would ask but a very short parley with '\n",
            " 'her own sheets an excellent precaution. You knew i should not wonder if it '\n",
            " 'were that it would be really having frank in their neighbourhood. Two such '\n",
            " 'could never be granted. The affection of sixteen years how she had played '\n",
            " 'with her from five years old how she had taught and how she had devoted all '\n",
            " 'her powers to attach and amuse her in health and spirits for going that they '\n",
            " 'made a point of some dignity as well as miss smith s conversation but she '\n",
            " 'found her decidedly more sensible than before of mr. Perhaps i intended you '\n",
            " 'to say and look every thing that look and manner could do made emma feel '\n",
            " 'that she had never been quite well since the time of year better weather and '\n",
            " 'that he knew how to do from the consciousness of there being no present '\n",
            " 'danger in returning home but no assurances could convince him that they '\n",
            " 'might in one of the sort of constant expectation there will be very bad. '\n",
            " 'Robert martin.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "from nltk.corpus import gutenberg\n",
        "import pprint\n",
        "\n",
        "# Run ngram on Emma (Jane Austen) and Macbeth (Shakespeare)\n",
        "emma_raw, macbeth_raw = gutenberg.raw('austen-emma.txt'), gutenberg.raw('shakespeare-macbeth.txt')\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "mac, emma, uni, bi = \"MACBETH\", \"EMMA\", \"UNIGRAM SENTENCES\", \"BIGRAM SENTENCES\"\n",
        "mac_uni, mac_bi = \" \".join([mac, uni]), \" \".join([mac, bi])\n",
        "emma_uni, emma_bi = \" \".join([emma, uni]), \" \".join([emma, bi])\n",
        "mac_five, emma_five = mac+\" 5-GRAM SENTENCES\", emma+\" 5-GRAM SENTENCES\"\n",
        "# Macbeth unigram-generated sentences:\n",
        "print('\\n')\n",
        "print(\"=\"*int((80-len(mac_uni))/2) + mac_uni + \"=\"*int((80-len(mac_uni))/2))\n",
        "pp.pprint(unigram_generate_sentences(macbeth_raw, 5))\n",
        "# Emma unigram-generated sentences:\n",
        "print('\\n')\n",
        "print(\"=\"*int((80-len(emma_uni))/2) + emma_uni + \"=\"*int((80-len(emma_uni))/2))\n",
        "pp.pprint(unigram_generate_sentences(emma_raw, 5))\n",
        "# Macbeth bigram-generated sentences:\n",
        "print('\\n')\n",
        "print(\"=\"*int((80-len(mac_bi))/2) + mac_bi + \"=\"*int((80-len(mac_bi))/2))\n",
        "pp.pprint(ngram_generate_sentences(macbeth_raw, 5, n=2))\n",
        "# Emma bigram-generated sentences\n",
        "print('\\n')\n",
        "print(\"=\"*int((80-len(emma_bi))/2) + emma_bi + \"=\"*int((80-len(emma_bi))/2))\n",
        "pp.pprint(ngram_generate_sentences(emma_raw, 5, n=2))\n",
        "# Macbeth 5-gram-generated sentences:\n",
        "print('\\n')\n",
        "print(\"=\"*int((80-len(mac_five))/2) + mac_five + \"=\"*int((80-len(mac_five))/2))\n",
        "pp.pprint(ngram_generate_sentences(macbeth_raw, 5, n=5))\n",
        "# Emma 5-gram-generated sentences\n",
        "print('\\n')\n",
        "print(\"=\"*int((80-len(emma_five))/2) + emma_five + \"=\"*int((80-len(emma_five))/2))\n",
        "pp.pprint(ngram_generate_sentences(emma_raw, 5, n=5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Bh03JUvBq4w",
        "outputId": "bc466c4c-f8b3-4275-f03c-3e8935169809"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "\n",
            "\n",
            "===========================MACBETH UNIGRAM SENTENCES===========================\n",
            "('Impediments passion must holy d report vpon domestique that to which our '\n",
            " 'large to you oh way will s so egge trauailing at it feare his they his caps '\n",
            " 'instant. S d proceed stay vnsure. Slaue dispatch haue vnity d stage the '\n",
            " 'without. Repetition tongue i me so vniust if last said come in should graspe '\n",
            " 'my dollars scotland what distinguishes the my we doe that. Their i skirre '\n",
            " 'and due macb more rosse it strike faces take him. ')\n",
            "\n",
            "\n",
            "=============================EMMA UNIGRAM SENTENCES=============================\n",
            "('Be myself dear. But not of however he only and handsome constitution of mrs '\n",
            " 'not acquiescence the you to to would. Life every she more had an more see '\n",
            " 'opinion seems expectation suspicions talk wish with their and engagement '\n",
            " 'alleviations our woodhouse be my good a and poor what her by and exactly was '\n",
            " 'be are to mr leaving great leaning of here of circle her s. Great on guess '\n",
            " 'wish happiest. You right. ')\n",
            "\n",
            "\n",
            "============================MACBETH BIGRAM SENTENCES============================\n",
            "(' Macb. Actus secundus. Enter seyton enter. Ring ministers where i could not '\n",
            " 'misse were bare fac d macb. O yet i flye thou art so exasperate their backs.')\n",
            "\n",
            "\n",
            "=============================EMMA BIGRAM SENTENCES=============================\n",
            "(' He were always been glad you truths while emma were bringing a noise. I '\n",
            " 'have done she was more delightful situation when you may say grandpapa can '\n",
            " 'be many good natured. As to give her visit and mrs. You as to half hour and '\n",
            " 'mrs. Let your journey through three things for the footpath a much less just '\n",
            " 'at ford s letters had the fashion.')\n",
            "\n",
            "\n",
            "============================MACBETH 5-GRAM SENTENCES============================\n",
            "(' Take thy face hence. Scena tertia. Hearke who lyes i th second chamber '\n",
            " 'lady. He had none his flight was madnesse when our actions do not our feares '\n",
            " 'do make vs traitors rosse. Giue me my armor seyt.')\n",
            "\n",
            "\n",
            "=============================EMMA 5-GRAM SENTENCES=============================\n",
            "(' Chapter xiii the weather continued much the same all the following morning '\n",
            " 'and the same loneliness and the same melancholy seemed to reign at hartfield '\n",
            " 'but in the afternoon it cleared the wind changed into a softer quarter the '\n",
            " 'clouds were carried off the sun appeared it was summer again. In spite of '\n",
            " 'the compliments she was receiving on her voice and her taste to look about '\n",
            " 'and see what became of mr. I think indeed said john knightley pleasantly '\n",
            " 'that mr. And as for objects of interest objects for the affections which is '\n",
            " 'in truth the great point of inferiority the want of which is really the '\n",
            " 'great evil to be avoided in not marrying i shall be very happy in spite of '\n",
            " 'the answer therefore she ordered the carriage and drove to mrs. Weston s '\n",
            " 'heart and time would be occupied by it.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.11**: Add an option to your program to compute the perplexity of a test set."
      ],
      "metadata": {
        "id": "pk0F1Ti5S0Ew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perplexity(corpus, words, n=2):\n",
        "  \"\"\"Compute perplexity of list words in corpus trained using ngram\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  corpus : str\n",
        "    corpus text\n",
        "  words : list\n",
        "    list of words to compute perplexity\n",
        "  n : int\n",
        "    parameter n of n-gram approximation\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  double\n",
        "    perplexity value\n",
        "  \"\"\"\n",
        "  N = len(words)\n",
        "  proc_corpus = process_corpus(corpus, n=n)\n",
        "  ngram_prob = ngram(proc_corpus, words, n)\n",
        "  if not ngram_prob:\n",
        "    return float('inf')\n",
        "  return ngram_prob**(-1/N)"
      ],
      "metadata": {
        "id": "4ULzvwj4RrmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "from nltk.corpus import gutenberg\n",
        "import pprint\n",
        "\n",
        "# Run ngram on Emma (Jane Austen)\n",
        "emma_raw = gutenberg.raw('austen-emma.txt')\n",
        "# Common phrase\n",
        "print(perplexity(emma_raw, [\"I\", \"think\"], 2))\n",
        "# Rare phrase\n",
        "print(perplexity(emma_raw, [\"the\", \"real\", \"evils\"], 2))\n"
      ],
      "metadata": {
        "id": "0uCRnKNQSo5V",
        "outputId": "61402fa5-d0f8-4341-9df5-6bbed7ab4073",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "5.436502143433365\n",
            "33.149611470929514\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.12**: You are given a training set of $100$ numbers that consists of $91$ zeros and $1$ each of the other digits $1-9$. Now we see the following test set: 0 0 0 0 0 3 0 0 0 0. What is the unigram perplexity?"
      ],
      "metadata": {
        "id": "QlgkusZqNayA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model trained using the skewed training set learns the following probabilities:\n",
        "\\begin{align}\n",
        "  P(0) &= .91 && &P(1) &= P(2) = \\dots = P(9) = .01\n",
        "\\end{align}\n",
        "\n",
        "Therefore, the unigram perplexity of the test set can be computed as follows:\n",
        "\\begin{align}\n",
        "  PP(W) &= P(0000030000)^{-\\frac{1}{10}} = \\left(P(0)^{9}P(3)\\right)^{-\\frac{1}{10}} \\approx 1.725\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "WqP4njuVNuJs"
      }
    }
  ]
}