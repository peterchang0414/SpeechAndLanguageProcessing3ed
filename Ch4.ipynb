{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peterchang0414/SpeechAndLanguageProcessing3ed/blob/main/Ch4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4: Naive Bayes and Sentiment Classification"
      ],
      "metadata": {
        "id": "qk47KCC8e3kD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solution by: Peter Chang (GItHub: @peterchang0414)\n",
        "\n",
        "*Last edited: 2022.1.21*"
      ],
      "metadata": {
        "id": "6A8G9Nch76Cq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.1**: Assume the following likelihoods for each word being part of a positive or negative movie review, and equal prior probabilities for each class.\n",
        "\\begin{array}{ c c c }\n",
        "  & \\text{pos} & \\text{neg} \\\\ \n",
        "  \\hline\n",
        "  \\text{I} & 0.09 & 0.16 \\\\  \n",
        "  \\text{always} & 0.07 & 0.06 \\\\\n",
        "  \\text{like} & 0.29 & 0.06 \\\\\n",
        "  \\text{foreign} & 0.04 & 0.15 \\\\\n",
        "  \\text{films} & 0.08 & 0.11   \n",
        "\\end{array}\n",
        "What class will Naive bayes assign to the sentence \"I always like foreign films.\"?"
      ],
      "metadata": {
        "id": "bxhs_F7yfDEK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using equation (4.10), since we assume equal prior probabilities for each class:\n",
        "\\begin{align}\n",
        "  c_{NB} = \\text{argmax}_{c \\in C}\\sum_{i \\in \\text{positions}} \\log{P(w_i|c)}\n",
        "\\end{align}\n",
        "Computing the log probabilities for the positive class:\n",
        "\\begin{align}\n",
        "  \\sum_{i \\in \\text{positions}} \\log{P(w_i|\\text{pos})} &= \\log(0.09) + \\dots + \\log(0.08) \\approx -5.233\n",
        "\\end{align}\n",
        "whereas for the negative class we have:\n",
        "\\begin{align}\n",
        "  \\sum_{i \\in \\text{positions}} \\log{P(w_i|\\text{neg})} &= \\log(0.16) + \\dots + \\log(0.11) \\approx -5.022\n",
        "\\end{align}\n",
        "Therefore, Naive bayes will assign the sentence to the negative class."
      ],
      "metadata": {
        "id": "c7IiC777E-rv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.2**: Given the following short movie reviews, each labeled with a genre, either comedy or action:\n",
        "\n",
        "\n",
        "1.   fun, couple, love, love: **comedy**\n",
        "2.   fast, furious, shoot: **action**\n",
        "3.   couple, fly, fast, fun, fun: **comedy**\n",
        "4.   furious, shoot, shoot, fun: **action**\n",
        "5.   fly, fast, shoot, love: **action**\n",
        "\n",
        "and a new document D:\n",
        "\n",
        "> fast, couple, shoot, fly\n",
        "\n",
        "compute the most likely class for D. Assume a naive Bayes classifier and use add-1 smoothing for the likelihoods."
      ],
      "metadata": {
        "id": "-TaqbgDr4G5e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The prior for the two classes is computed:\n",
        "$$\n",
        "\\begin{align}\n",
        "  P(\\text{comedy}) &= \\frac{2}{5}, & P(\\text{action}) &= \\frac{3}{5}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "The probabilities for the words in the document D are as follows:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "  P(\\text{fast}|\\text{comedy}) &= \\frac{1+1}{9+7}=\\frac{2}{16}, & P(\\text{fast}|\\text{action}) &= \\frac{2+1}{11+7}=\\frac{3}{18} \\\\\n",
        "  P(\\text{couple}|\\text{comedy}) &= \\frac{2+1}{9+7}=\\frac{3}{16}, & P(\\text{couple}|\\text{action}) &= \\frac{0+1}{11+7}=\\frac{1}{18} \\\\\n",
        "  P(\\text{shoot}|\\text{comedy}) &= \\frac{0+1}{9+7}=\\frac{1}{16}, & P(\\text{shoot}|\\text{action}) &= \\frac{4+1}{11+7}=\\frac{5}{18} \\\\\n",
        "  P(\\text{fly}|\\text{comedy}) &= \\frac{1+1}{9+7}=\\frac{2}{16}, & P(\\text{fly}|\\text{action}) &= \\frac{1+1}{11+7}=\\frac{2}{18}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Therefore, the posterior probabilities are computed:\n",
        "$$\n",
        "\\begin{align}\n",
        "  P(\\text{comedy})P(D|\\text{comedy}) &= \\frac{2}{5} \\times \\frac{12}{16^{4}} \\approx 7.3 \\times 10^{-5} \\\\\n",
        "  P(\\text{action})P(D|\\text{action}) &= \\frac{3}{5} \\times \\frac{30}{18^{4}} \\approx 1.7 \\times 10^{-4}\n",
        "\\end{align}\n",
        "$$\n",
        "The model thus predicts the class *action* for the document $D$."
      ],
      "metadata": {
        "id": "-pjM-hQVkoeU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.3**: Train two models, multinomial naive Bayes and binarized naive Bayes, both with add-1 smoothing, on the following document counts for key sentiment words, with positive or negative class assigned as noted.\n",
        "\n",
        "$$\n",
        "\\begin{array}{ l l l l l}\n",
        " \\text{doc} & ``\\text{good\"} & ``\\text{poor\"} & ``\\text{great\"}&(\\text{class})\\\\ \n",
        " \\text{d1.} & 3 & 0 & 3 & \\text{pos}\\\\  \n",
        " \\text{d2.} & 0 & 1 & 2 & \\text{pos}\\\\\n",
        " \\text{d3.} & 1 & 3 & 0 & \\text{neg}\\\\\n",
        " \\text{d4.} & 1 & 5 & 2 & \\text{neg}\\\\\n",
        " \\text{d5.} & 0 & 2 & 0 & \\text{neg}\n",
        "\\end{array}\n",
        "$$\n",
        "Use both naive Bayes models to assign a class (pos or neg) to this sentence:\n",
        "\n",
        "> A good, good plot and great characters, but poor acting.\n",
        "\n",
        "Recall from page 62 that with naive Bayes text classification, we simply ignore (throw out) any word that never occurred in the training document. (We don't throw out words that appear in some classes but not others; that's what add-one smoothing is for.) Do the two models agree or disagree?\n"
      ],
      "metadata": {
        "id": "sxJSBAl2xxGx"
      }
    }
  ]
}